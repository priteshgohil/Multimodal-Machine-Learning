{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "if('/opt/ros/kinetic/lib/python2.7/dist-packages' in sys.path):\n",
    "    sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# base class for dataloader vision.py\n",
    "\n",
    "\n",
    "class VisionDataset(data.Dataset):\n",
    "    _repr_indent = 4\n",
    "\n",
    "    def __init__(self, root, transforms=None, transform=None, target_transform=None):\n",
    "        if isinstance(root, torch._six.string_classes):\n",
    "            root = os.path.expanduser(root)\n",
    "        self.root = root\n",
    "\n",
    "        has_transforms = transforms is not None\n",
    "        has_separate_transform = transform is not None or target_transform is not None\n",
    "        if has_transforms and has_separate_transform:\n",
    "            raise ValueError(\"Only transforms or transform/target_transform can \"\n",
    "                             \"be passed as argument\")\n",
    "\n",
    "        # for backwards-compatibility\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        if has_separate_transform:\n",
    "            transforms = StandardTransform(transform, target_transform)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __repr__(self):\n",
    "        head = \"Dataset \" + self.__class__.__name__\n",
    "        body = [\"Number of datapoints: {}\".format(self.__len__())]\n",
    "        if self.root is not None:\n",
    "            body.append(\"Root location: {}\".format(self.root))\n",
    "        body += self.extra_repr().splitlines()\n",
    "        if hasattr(self, \"transforms\") and self.transforms is not None:\n",
    "            body += [repr(self.transforms)]\n",
    "        lines = [head] + [\" \" * self._repr_indent + line for line in body]\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def _format_transform_repr(self, transform, head):\n",
    "        lines = transform.__repr__().splitlines()\n",
    "        return ([\"{}{}\".format(head, lines[0])] +\n",
    "                [\"{}{}\".format(\" \" * len(head), line) for line in lines[1:]])\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "class StandardTransform(object):\n",
    "    def __init__(self, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __call__(self, input, target):\n",
    "        if self.transform is not None:\n",
    "            input = self.transform(input)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return input, target\n",
    "\n",
    "    def _format_transform_repr(self, transform, head):\n",
    "        lines = transform.__repr__().splitlines()\n",
    "        return ([\"{}{}\".format(head, lines[0])] +\n",
    "                [\"{}{}\".format(\" \" * len(head), line) for line in lines[1:]])\n",
    "\n",
    "    def __repr__(self):\n",
    "        body = [self.__class__.__name__]\n",
    "        if self.transform is not None:\n",
    "            body += self._format_transform_repr(self.transform,\n",
    "                                                \"Transform: \")\n",
    "        if self.target_transform is not None:\n",
    "            body += self._format_transform_repr(self.target_transform,\n",
    "                                                \"Target transform: \")\n",
    "\n",
    "        return '\\n'.join(body)\n",
    "\n",
    "\n",
    "FILE_EXTENSIONS = ('front_0.jpg','tactile.txt','label.txt','front_rgb.mp4','left_rgb.mp4', 'pos.txt')\n",
    "TACTILE_MAX_MAGNITUDE = 75431.01\n",
    "\n",
    "def get_params(img, output_size):\n",
    "    \"\"\"random crop input sequence of frames.\n",
    "    Args:\n",
    "        img (PIL Image): Image to be cropped.\n",
    "        output_size (tuple): Expected output size of the crop.\n",
    "    Returns:\n",
    "        tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n",
    "    \"\"\"\n",
    "    t, h, w, c = img.shape\n",
    "    th, tw = output_size\n",
    "    if w == tw and h == th:\n",
    "        return 0, 0, h, w\n",
    "\n",
    "    i = random.randint(0, h - th) if h!=th else 0\n",
    "    j = random.randint(0, w - tw) if w!=tw else 0\n",
    "    return i, j, th, tw\n",
    "\n",
    "def random_crop(imgs):\n",
    "    i, j, h, w = get_params(a, (224,224))\n",
    "    imgs = imgs[:, i:i+h, j:j+w, :]\n",
    "    return imgs\n",
    "\n",
    "def tactile_loader(path, frames):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    output =[] \n",
    "    TACTILE_TIME = 24\n",
    "    UPSAMPLE_FREQ = 18\n",
    "    up_samples=TACTILE_TIME*UPSAMPLE_FREQ\n",
    "    with open(path, 'rb') as f:\n",
    "        tactile_frame = pd.read_csv(f,delimiter=' ', header=None)\n",
    "        tactile = tactile_frame.as_matrix()\n",
    "        tactile = tactile.astype('float')\n",
    "        tactile = signal.resample(tactile,up_samples)\n",
    "        tactile[np.where(tactile<0.1)] = 0.  # remove all the negative samples with 0\n",
    "        tactile = tactile/TACTILE_MAX_MAGNITUDE #normalize input in range of 0 to 1.\n",
    "        for frame in frames:\n",
    "            output.append(tactile[frame,:].reshape(4,4,1))\n",
    "        return np.array(output)\n",
    "\n",
    "def video_to_tensor(frames):\n",
    "    \"\"\"Convert a ``numpy.ndarray`` to tensor.\n",
    "    Converts a numpy.ndarray (T x H x W x C)\n",
    "    to a torch.FloatTensor of shape (C x T x H x W)\n",
    "\n",
    "    Args:\n",
    "         pic (numpy.ndarray): Video to be converted to tensor.\n",
    "    Returns:\n",
    "         Tensor: Converted video.\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(frames.transpose([3,0,1,2]))\n",
    "\n",
    "def compress_image(cv_frame, size):\n",
    "    \"\"\"\n",
    "    Args:   frame - cv image\n",
    "            size - tuple of (W x H)\n",
    "    \"\"\"\n",
    "    return cv2.resize(cv_frame, size)\n",
    "\n",
    "def preprocess_image(cv_frame, size):\n",
    "    \"\"\"\n",
    "    Args:   frame - cv image\n",
    "            size - tuple of (W x H)\n",
    "    \"\"\"\n",
    "    im = cv2.resize(cv_frame, size) #compress image\n",
    "    im = np.array(Image.fromarray(cv2.cvtColor(im,cv2.COLOR_BGR2RGB))) #convert to RGB image\n",
    "    im = im/255 #normalize\n",
    "    im = (im - 0.5)/0.5\n",
    "    return im\n",
    "    \n",
    "def vid_loader(path):\n",
    "    \"\"\"\n",
    "    1. Sample 64 frames from the video at equal interval.\n",
    "    2. Compress image from (640 x 480) to (320 x 240)\n",
    "    3.\n",
    "    3. Use Random crop to get (224 x 224)\n",
    "    Args:\n",
    "        dir (string): Root directory path.\n",
    "    Returns:\n",
    "        tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
    "    Ensures:\n",
    "        No class is a subdirectory of another.\n",
    "    \"\"\"\n",
    "    #print(path)\n",
    "    vidcap = cv2.VideoCapture(path)\n",
    "    success,image = vidcap.read()\n",
    "    count = 0\n",
    "    required_frames = 64\n",
    "    frame_output = 5\n",
    "    # 18x18 = 324  frames in total\n",
    "    length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     print(\"length\",length)\n",
    "    fps = int(vidcap.get(cv2.CAP_PROP_FPS))\n",
    "#     print(\"FPS\", fps )\n",
    "    duration = length / fps\n",
    "#     print(\"duration\",duration)\n",
    "\n",
    "    images = []\n",
    "    frames = []\n",
    "    while success:\n",
    "        if((count%5)==0 and (len(images)<required_frames)):\n",
    "            image = preprocess_image(image, (320,240))\n",
    "            images.append(image)\n",
    "            frames.append(count)\n",
    "        success,image = vidcap.read()\n",
    "        count += 1\n",
    "#     print(\"normal len : \" ,len(images))\n",
    "    missing_frames = required_frames - len(images)\n",
    "#     print(\"missing frames : \" ,missing_frames)\n",
    "    if(missing_frames):\n",
    "        remove_element = (missing_frames//frame_output)+1\n",
    "        del images[-remove_element:]\n",
    "        del frames[-remove_element:]\n",
    "        vidcap = cv2.VideoCapture(path)\n",
    "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, length-missing_frames-remove_element)\n",
    "        success,image = vidcap.read()\n",
    "        temp_count = 0\n",
    "        while success:\n",
    "            image = preprocess_image(image, (320,240))\n",
    "            images.append(image)\n",
    "            frames.append(length-missing_frames-remove_element+temp_count)\n",
    "            success,image = vidcap.read()\n",
    "            temp_count +=1\n",
    "    while(len(images) is not required_frames):\n",
    "        vidcap = cv2.VideoCapture(path)\n",
    "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, length-1)\n",
    "        success,image = vidcap.read()\n",
    "        image = preprocess_image(image, (320,240))\n",
    "        images.append(image)\n",
    "        frames.append(length-1)\n",
    "    return np.array(images),np.array(frames)\n",
    "\n",
    "def preprocess_grayscale_image(cv_frame, size):\n",
    "    \"\"\"\n",
    "    Args:   frame - cv image\n",
    "            size - tuple of (W x H)\n",
    "    \"\"\"\n",
    "    im = cv2.resize(cv_frame, size) #compress image\n",
    "    im = im/255 #normalize\n",
    "    im = (im - 0.5)/0.5\n",
    "    return im\n",
    "\n",
    "def flow_loader(path, frames):\n",
    "    flow_x = []\n",
    "    flow_y = []\n",
    "    images = []\n",
    "    images_x = []\n",
    "    images_y = []\n",
    "    count = 0\n",
    "    required_frames = 64\n",
    "    for img in os.listdir(path):\n",
    "        if img.endswith('x.png'):\n",
    "            flow_x.append(img)\n",
    "        else:\n",
    "            flow_y.append(img)\n",
    "    flow_x = sorted(flow_x)\n",
    "    flow_y = sorted(flow_y)\n",
    "    for frame in frames:\n",
    "        while True:\n",
    "            if(frame==int(flow_x[count][6:10])):\n",
    "                images_x.append(flow_x[count])\n",
    "                images_y.append(flow_y[count])\n",
    "                break\n",
    "            else:\n",
    "                count += 1\n",
    "    for x,y in zip(images_x,images_y):\n",
    "        img_x = cv2.imread(path + x, cv2.IMREAD_GRAYSCALE)\n",
    "        img_x = preprocess_grayscale_image(img_x,(320,240))\n",
    "        img_y = cv2.imread(path + y, cv2.IMREAD_GRAYSCALE)\n",
    "        img_y = preprocess_grayscale_image(img_y,(320,240))\n",
    "        images.append(np.stack((img_x,img_y)).transpose([1,2,0])) #output shape T x H x W x C\n",
    "    if(len(images) is not required_frames):\n",
    "        raise ValueError(\"missing flow frames\")\n",
    "    return np.array(images)\n",
    "    \n",
    "#     count = 0\n",
    "#     required_frames = 64\n",
    "#     frame_output = 5\n",
    "#     length = len(flow_x)\n",
    "#     print(flow_x)\n",
    "#     flow_x_list = flow_x[::frame_output]\n",
    "#     flow_y_list = flow_y[::frame_output]\n",
    "\n",
    "#     missing_frames = required_frames - len(flow_x_list)\n",
    "#     if(missing_frames>0):\n",
    "#         remove_element = (missing_frames//frame_output)+1\n",
    "#         del flow_x_list[-remove_element:]\n",
    "#         del flow_y_list[-remove_element:]\n",
    "#         missing_frames = required_frames - len(flow_x_list)\n",
    "#         [flow_x_list.append(i) for i in flow_x[-missing_frames:]]\n",
    "#         [flow_y_list.append(i) for i in flow_y[-missing_frames:]]\n",
    "#     elif(missing_frames<0): #contains extra frames and need to remove\n",
    "#         del flow_x_list[-abs(missing_frames):]\n",
    "#         del flow_y_list[-abs(missing_frames):]\n",
    "#     images = []    \n",
    "#     for x,y in zip(flow_x_list,flow_y_list):\n",
    "#         img_x = cv2.imread(path + x, cv2.IMREAD_GRAYSCALE)\n",
    "#         img_x = preprocess_grayscale_image(img_x,(320,240))\n",
    "#         img_y = cv2.imread(path + y, cv2.IMREAD_GRAYSCALE)\n",
    "#         img_y = preprocess_grayscale_image(img_y,(320,240))\n",
    "#         images.append(np.stack((img_x,img_y)).transpose([1,2,0])) #output shape T x H x W x C\n",
    "#     return np.array(images)\n",
    "\n",
    "def old_flow_loader(path):\n",
    "    flow_x = []\n",
    "    flow_y = []\n",
    "    for img in os.listdir(path):\n",
    "        if img.endswith('x.png'):\n",
    "            flow_x.append(img)\n",
    "        else:\n",
    "            flow_y.append(img)\n",
    "    flow_x = sorted(flow_x)\n",
    "    flow_y = sorted(flow_y)\n",
    "    count = 0\n",
    "    required_frames = 64\n",
    "    frame_output = 5\n",
    "    length = len(flow_x)\n",
    "\n",
    "    flow_x_list = flow_x[::frame_output]\n",
    "    flow_y_list = flow_y[::frame_output]\n",
    "\n",
    "    missing_frames = required_frames - len(flow_x_list)\n",
    "    if(missing_frames>0):\n",
    "        remove_element = (missing_frames//frame_output)+1\n",
    "        del flow_x_list[-remove_element:]\n",
    "        del flow_y_list[-remove_element:]\n",
    "        missing_frames = required_frames - len(flow_x_list)\n",
    "        [flow_x_list.append(i) for i in flow_x[-missing_frames:]]\n",
    "        [flow_y_list.append(i) for i in flow_y[-missing_frames:]]\n",
    "    elif(missing_frames<0): #contains extra frames and need to remove\n",
    "        del flow_x_list[-abs(missing_frames):]\n",
    "        del flow_y_list[-abs(missing_frames):]\n",
    "    images = []    \n",
    "    for x,y in zip(flow_x_list,flow_y_list):\n",
    "        img_x = cv2.imread(path + x, cv2.IMREAD_GRAYSCALE)\n",
    "        img_x = preprocess_grayscale_image(img_x,(320,240))\n",
    "        img_y = cv2.imread(path + y, cv2.IMREAD_GRAYSCALE)\n",
    "        img_y = preprocess_grayscale_image(img_y,(320,240))\n",
    "        images.append(np.stack((img_x,img_y)).transpose([1,2,0])) #output shape T x H x W x C\n",
    "    return np.array(images)\n",
    "\"\"\"\n",
    "Make dataset with following input arguments:\n",
    "split_file : .txt file containing path to each modalities to be trained\n",
    "root: dataset folder path\n",
    "mode: decides how many samples we need in the output Ex: \"RGB\" or \"V\", \"FLOW\" or \"F\", \"VF\",\n",
    "        \"VFT\", \"VFTP\" (video, flow, tactile, position)\n",
    "\"\"\"\n",
    "def make_dataset(split_file, root, mode, transform):\n",
    "    dataset = []\n",
    "    file_path = np.loadtxt(split_file,dtype=str)\n",
    "    prefix = root.split(file_path[0][:32])[0]\n",
    "    file_names = ['front_rgb.mp4','left_rgb.mp4','tactile.txt','pos.txt','label.txt','flow']\n",
    "#     print(prefix+file_path[0]+file_names[0])\n",
    "    for i,file in enumerate(file_path):\n",
    "        front_video_path = prefix + file + file_names[0]\n",
    "        left_video_path = prefix + file + file_names[1]\n",
    "        tactile_path = prefix + file + file_names[2]\n",
    "        pos_path = prefix + file + file_names[3]\n",
    "        label_path = prefix + file + file_names[4]\n",
    "        front_flow_path = prefix + file + file_names[5] + '/' + 'front_rgb/'\n",
    "        left_flow_path = prefix + file + file_names[5] + '/' + 'left_rgb/'\n",
    "        dataset.append((front_video_path, left_video_path, tactile_path, pos_path, \\\n",
    "                       front_flow_path, left_flow_path, np.loadtxt(label_path)[3]))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "class VisualTactile(VisionDataset):\n",
    "    \"\"\"Dataloader customized to load Visual-Tactile dataset\n",
    "    Args:\n",
    "        split_file (string): path to .txt file where we have path to each test and train object.\n",
    "        root (string): Root directory path.\n",
    "        mode (string) : For future implementation. (Decide which data to load V, F, VF, VFT, VFTP)\n",
    "        transform (callable, optional): A function/transform that takes in\n",
    "            a sample and returns a transformed version.\n",
    "            E.g, ``transforms.RandomCrop`` for images.\n",
    "     Attributes:\n",
    "        samples (list): List of (sample path, class_index) tuples\n",
    "        targets (list): The class_index value for each image in the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split_file, root, mode=None, transform=None):\n",
    "        super(VisualTactile, self).__init__(root)\n",
    "\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.split_file = split_file\n",
    "        self.transform = transform\n",
    "        self.extensions = FILE_EXTENSIONS\n",
    "\n",
    "        # classes = name of the folder\n",
    "        # class_to_idx = dictionary with class name(folder name) and index\n",
    "        \"\"\"classes, class_to_idx = self._find_classes(self.root)\"\"\"\n",
    "        self.samples = make_dataset(self.split_file, self.root, self.mode, self.transform)\n",
    "#         self.samples = old_make_dataset(self.root, class_to_idx, extensions, is_valid_file)\n",
    "#         print(\"Samaples\" , self.samples[0])\n",
    "        if len(self.samples) == 0:\n",
    "            raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n",
    "                                \"Supported extensions are: \" + \",\".join(extensions)))\n",
    "\n",
    "        \"\"\"self.classes = classes\n",
    "        self.class_to_idx = class_to_idx\"\"\"\n",
    "        self.targets = [s[-1] for s in self.samples]\n",
    "\n",
    "    def _find_classes(self, dir):\n",
    "        \"\"\"\n",
    "        Finds the class folders in a dataset.\n",
    "        Args:\n",
    "            dir (string): Root directory path.\n",
    "        Returns:\n",
    "            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
    "        Ensures:\n",
    "            No class is a subdirectory of another.\n",
    "        \"\"\"\n",
    "        if sys.version_info >= (3, 5):\n",
    "            # Faster and available in Python 3.5 and above\n",
    "            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
    "        else:\n",
    "            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "        classes.sort()\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (f_video, l_video, tactile, pos, f_flow, l_flow, target) where target is successfull pickup or not.\n",
    "        \"\"\"\n",
    "        #add conditoin here wich will return only the requested mode. Ex. V, or VFTP\n",
    "        f_video, l_video, tactile, pos, f_flow, l_flow, target = self.samples[index]\n",
    "        sample_f_video, selected_frames = vid_loader(f_video)\n",
    "#         sample_l_video = vid_loader(l_video)\n",
    "        sample_tactile = tactile_loader(tactile, selected_frames)\n",
    "#         sample_pos = tactile_loader(pos)\n",
    "        sample_target = target\n",
    "#         sample_f_flow = flow_loader(f_flow, selected_frames)\n",
    "#         sample_l_flow = flow_loader(l_flow)\n",
    "        if self.transform is not None:\n",
    "            sample_f_video = self.transform(sample_f_video)\n",
    "            sample_l_video = self.transform(sample_l_video)\n",
    "#         return sample_f_video, sample_l_video, sample_tactile, sample_pos, sample_target\n",
    "#         return video_to_tensor(sample_f_video), video_to_tensor(sample_l_video), \\\n",
    "#                 torch.from_numpy(sample_tactile), torch.from_numpy(sample_pos), \\\n",
    "#                 video_to_tensor(sample_f_flow), video_to_tensor(sample_l_flow), \\\n",
    "#                 torch.from_numpy(np.array(sample_target))\n",
    "#         return video_to_tensor(sample_f_video), video_to_tensor(sample_l_video), \\\n",
    "#                 torch.from_numpy(sample_tactile), torch.from_numpy(sample_pos), \\\n",
    "#                 torch.from_numpy(np.array(sample_target))\n",
    "        return video_to_tensor(sample_tactile), sample_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "class VisualTactileFolder(VisualTactile):\n",
    "    \"\"\"A generic data loader where the images are arranged in this way: ::\n",
    "        root/dog/xxx.png\n",
    "        root/dog/xxy.png\n",
    "        root/dog/xxz.png\n",
    "        root/cat/123.png\n",
    "        root/cat/nsdf3.png\n",
    "        root/cat/asd932_.png\n",
    "    Args:\n",
    "        root (string): Root directory path.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        loader (callable, optional): A function to load an image given its path.\n",
    "        is_valid_file (callable, optional): A function that takes path of an Image file\n",
    "            and check if the file is a valid_file (used to check of corrupt files)\n",
    "     Attributes:\n",
    "        classes (list): List of the class names.\n",
    "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
    "        imgs (list): List of (image path, class_index) tuples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split_file, root, mode=None, transform=None):\n",
    "        super(VisualTactileFolder, self).__init__(split_file, root, mode=None, transform=None)\n",
    "        self.imgs = self.samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root = '/media/pritesh/Entertainment/Visual-Tactile_Dataset/dataset/'\n",
    "ds = VisualTactile('../master_i3d/testv2.txt', root)\n",
    "ds_train = VisualTactile('../master_i3d/trainv2.txt', root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds_train.samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. No upsampling, new tactile data folder, normalized by channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "if('/opt/ros/kinetic/lib/python2.7/dist-packages' in sys.path):\n",
    "    sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# base class for dataloader vision.py\n",
    "\n",
    "\n",
    "class VisionDataset(data.Dataset):\n",
    "    _repr_indent = 4\n",
    "\n",
    "    def __init__(self, root, transforms=None, transform=None, target_transform=None):\n",
    "        if isinstance(root, torch._six.string_classes):\n",
    "            root = os.path.expanduser(root)\n",
    "        self.root = root\n",
    "\n",
    "        has_transforms = transforms is not None\n",
    "        has_separate_transform = transform is not None or target_transform is not None\n",
    "        if has_transforms and has_separate_transform:\n",
    "            raise ValueError(\"Only transforms or transform/target_transform can \"\n",
    "                             \"be passed as argument\")\n",
    "\n",
    "        # for backwards-compatibility\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        if has_separate_transform:\n",
    "            transforms = StandardTransform(transform, target_transform)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __repr__(self):\n",
    "        head = \"Dataset \" + self.__class__.__name__\n",
    "        body = [\"Number of datapoints: {}\".format(self.__len__())]\n",
    "        if self.root is not None:\n",
    "            body.append(\"Root location: {}\".format(self.root))\n",
    "        body += self.extra_repr().splitlines()\n",
    "        if hasattr(self, \"transforms\") and self.transforms is not None:\n",
    "            body += [repr(self.transforms)]\n",
    "        lines = [head] + [\" \" * self._repr_indent + line for line in body]\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def _format_transform_repr(self, transform, head):\n",
    "        lines = transform.__repr__().splitlines()\n",
    "        return ([\"{}{}\".format(head, lines[0])] +\n",
    "                [\"{}{}\".format(\" \" * len(head), line) for line in lines[1:]])\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "class StandardTransform(object):\n",
    "    def __init__(self, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __call__(self, input, target):\n",
    "        if self.transform is not None:\n",
    "            input = self.transform(input)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return input, target\n",
    "\n",
    "    def _format_transform_repr(self, transform, head):\n",
    "        lines = transform.__repr__().splitlines()\n",
    "        return ([\"{}{}\".format(head, lines[0])] +\n",
    "                [\"{}{}\".format(\" \" * len(head), line) for line in lines[1:]])\n",
    "\n",
    "    def __repr__(self):\n",
    "        body = [self.__class__.__name__]\n",
    "        if self.transform is not None:\n",
    "            body += self._format_transform_repr(self.transform,\n",
    "                                                \"Transform: \")\n",
    "        if self.target_transform is not None:\n",
    "            body += self._format_transform_repr(self.target_transform,\n",
    "                                                \"Target transform: \")\n",
    "\n",
    "        return '\\n'.join(body)\n",
    "\n",
    "\n",
    "FILE_EXTENSIONS = ('front_0.jpg','tactile.txt','label.txt','front_rgb.mp4','left_rgb.mp4', 'pos.txt')\n",
    "TACTILE_MAX_MAGNITUDE = np.array([23655, 20662, 14496,  6475, 41133, 64793, 59317, 33177, 19897,\n",
    "       62084, 49874, 29170, 42944, 14976, 12311, 14331])\n",
    "\n",
    "def get_params(img, output_size):\n",
    "    \"\"\"random crop input sequence of frames.\n",
    "    Args:\n",
    "        img (PIL Image): Image to be cropped.\n",
    "        output_size (tuple): Expected output size of the crop.\n",
    "    Returns:\n",
    "        tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n",
    "    \"\"\"\n",
    "    t, h, w, c = img.shape\n",
    "    th, tw = output_size\n",
    "    if w == tw and h == th:\n",
    "        return 0, 0, h, w\n",
    "\n",
    "    i = random.randint(0, h - th) if h!=th else 0\n",
    "    j = random.randint(0, w - tw) if w!=tw else 0\n",
    "    return i, j, th, tw\n",
    "\n",
    "def random_crop(imgs):\n",
    "    i, j, h, w = get_params(a, (224,224))\n",
    "    imgs = imgs[:, i:i+h, j:j+w, :]\n",
    "    return imgs\n",
    "\n",
    "def tactile_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    output =[]\n",
    "    groups = []\n",
    "    tactile_frame = np.loadtxt(path)\n",
    "    tactile = tactile_frame.astype('float')\n",
    "#     tactile = tactile/TACTILE_MAX_MAGNITUDE\n",
    "#     #group of 4 sensors\n",
    "#     n=4\n",
    "#     out = [tactile[:,k:k+n] for k in range(0, tactile.shape[1], n)]\n",
    "#     for data in out:\n",
    "#         for i in range(data.shape[1]):\n",
    "#             groups.append(tactile[:324,i].reshape(18,18,1))\n",
    "#         temp = video_to_tensor(np.array(groups))\n",
    "#         output.append(temp)\n",
    "#         groups = []\n",
    "    return tactile\n",
    "\n",
    "def video_to_tensor(frames):\n",
    "    \"\"\"Convert a ``numpy.ndarray`` to tensor.\n",
    "    Converts a numpy.ndarray (T x H x W x C)\n",
    "    to a torch.FloatTensor of shape (C x T x H x W)\n",
    "\n",
    "    Args:\n",
    "         pic (numpy.ndarray): Video to be converted to tensor.\n",
    "    Returns:\n",
    "         Tensor: Converted video.\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(frames.transpose([3,0,1,2]))\n",
    "\n",
    "def compress_image(cv_frame, size):\n",
    "    \"\"\"\n",
    "    Args:   frame - cv image\n",
    "            size - tuple of (W x H)\n",
    "    \"\"\"\n",
    "    return cv2.resize(cv_frame, size)\n",
    "\n",
    "def preprocess_image(cv_frame, size):\n",
    "    \"\"\"\n",
    "    Args:   frame - cv image\n",
    "            size - tuple of (W x H)\n",
    "    \"\"\"\n",
    "    im = cv2.resize(cv_frame, size) #compress image\n",
    "    im = np.array(Image.fromarray(cv2.cvtColor(im,cv2.COLOR_BGR2RGB))) #convert to RGB image\n",
    "    im = im/255 #normalize\n",
    "    im = (im - 0.5)/0.5\n",
    "    return im\n",
    "    \n",
    "def vid_loader(path):\n",
    "    \"\"\"\n",
    "    1. Sample 64 frames from the video at equal interval.\n",
    "    2. Compress image from (640 x 480) to (320 x 240)\n",
    "    3.\n",
    "    3. Use Random crop to get (224 x 224)\n",
    "    Args:\n",
    "        dir (string): Root directory path.\n",
    "    Returns:\n",
    "        tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
    "    Ensures:\n",
    "        No class is a subdirectory of another.\n",
    "    \"\"\"\n",
    "    #print(path)\n",
    "    vidcap = cv2.VideoCapture(path)\n",
    "    success,image = vidcap.read()\n",
    "    count = 0\n",
    "    required_frames = 64\n",
    "    frame_output = 5\n",
    "    # 18x18 = 324  frames in total\n",
    "    length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     print(\"length\",length)\n",
    "    fps = int(vidcap.get(cv2.CAP_PROP_FPS))\n",
    "#     print(\"FPS\", fps )\n",
    "    duration = length / fps\n",
    "#     print(\"duration\",duration)\n",
    "\n",
    "    images = []\n",
    "    frames = []\n",
    "    while success:\n",
    "        if((count%5)==0 and (len(images)<required_frames)):\n",
    "            image = preprocess_image(image, (320,240))\n",
    "            images.append(image)\n",
    "            frames.append(count)\n",
    "        success,image = vidcap.read()\n",
    "        count += 1\n",
    "#     print(\"normal len : \" ,len(images))\n",
    "    missing_frames = required_frames - len(images)\n",
    "#     print(\"missing frames : \" ,missing_frames)\n",
    "    if(missing_frames):\n",
    "        remove_element = (missing_frames//frame_output)+1\n",
    "        del images[-remove_element:]\n",
    "        del frames[-remove_element:]\n",
    "        vidcap = cv2.VideoCapture(path)\n",
    "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, length-missing_frames-remove_element)\n",
    "        success,image = vidcap.read()\n",
    "        temp_count = 0\n",
    "        while success:\n",
    "            image = preprocess_image(image, (320,240))\n",
    "            images.append(image)\n",
    "            frames.append(length-missing_frames-remove_element+temp_count)\n",
    "            success,image = vidcap.read()\n",
    "            temp_count +=1\n",
    "    while(len(images) is not required_frames):\n",
    "        vidcap = cv2.VideoCapture(path)\n",
    "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, length-1)\n",
    "        success,image = vidcap.read()\n",
    "        image = preprocess_image(image, (320,240))\n",
    "        images.append(image)\n",
    "        frames.append(length-1)\n",
    "    return np.array(images),np.array(frames)\n",
    "\n",
    "def preprocess_grayscale_image(cv_frame, size):\n",
    "    \"\"\"\n",
    "    Args:   frame - cv image\n",
    "            size - tuple of (W x H)\n",
    "    \"\"\"\n",
    "    im = cv2.resize(cv_frame, size) #compress image\n",
    "    im = im/255 #normalize\n",
    "    im = (im - 0.5)/0.5\n",
    "    return im\n",
    "\n",
    "def flow_loader(path, frames):\n",
    "    flow_x = []\n",
    "    flow_y = []\n",
    "    images = []\n",
    "    images_x = []\n",
    "    images_y = []\n",
    "    count = 0\n",
    "    required_frames = 64\n",
    "    for img in os.listdir(path):\n",
    "        if img.endswith('x.png'):\n",
    "            flow_x.append(img)\n",
    "        else:\n",
    "            flow_y.append(img)\n",
    "    flow_x = sorted(flow_x)\n",
    "    flow_y = sorted(flow_y)\n",
    "    for frame in frames:\n",
    "        while True:\n",
    "            if(frame==int(flow_x[count][6:10])):\n",
    "                images_x.append(flow_x[count])\n",
    "                images_y.append(flow_y[count])\n",
    "                break\n",
    "            else:\n",
    "                count += 1\n",
    "    for x,y in zip(images_x,images_y):\n",
    "        img_x = cv2.imread(path + x, cv2.IMREAD_GRAYSCALE)\n",
    "        img_x = preprocess_grayscale_image(img_x,(320,240))\n",
    "        img_y = cv2.imread(path + y, cv2.IMREAD_GRAYSCALE)\n",
    "        img_y = preprocess_grayscale_image(img_y,(320,240))\n",
    "        images.append(np.stack((img_x,img_y)).transpose([1,2,0])) #output shape T x H x W x C\n",
    "    if(len(images) is not required_frames):\n",
    "        raise ValueError(\"missing flow frames\")\n",
    "    return np.array(images)\n",
    "    \n",
    "#     count = 0\n",
    "#     required_frames = 64\n",
    "#     frame_output = 5\n",
    "#     length = len(flow_x)\n",
    "#     print(flow_x)\n",
    "#     flow_x_list = flow_x[::frame_output]\n",
    "#     flow_y_list = flow_y[::frame_output]\n",
    "\n",
    "#     missing_frames = required_frames - len(flow_x_list)\n",
    "#     if(missing_frames>0):\n",
    "#         remove_element = (missing_frames//frame_output)+1\n",
    "#         del flow_x_list[-remove_element:]\n",
    "#         del flow_y_list[-remove_element:]\n",
    "#         missing_frames = required_frames - len(flow_x_list)\n",
    "#         [flow_x_list.append(i) for i in flow_x[-missing_frames:]]\n",
    "#         [flow_y_list.append(i) for i in flow_y[-missing_frames:]]\n",
    "#     elif(missing_frames<0): #contains extra frames and need to remove\n",
    "#         del flow_x_list[-abs(missing_frames):]\n",
    "#         del flow_y_list[-abs(missing_frames):]\n",
    "#     images = []    \n",
    "#     for x,y in zip(flow_x_list,flow_y_list):\n",
    "#         img_x = cv2.imread(path + x, cv2.IMREAD_GRAYSCALE)\n",
    "#         img_x = preprocess_grayscale_image(img_x,(320,240))\n",
    "#         img_y = cv2.imread(path + y, cv2.IMREAD_GRAYSCALE)\n",
    "#         img_y = preprocess_grayscale_image(img_y,(320,240))\n",
    "#         images.append(np.stack((img_x,img_y)).transpose([1,2,0])) #output shape T x H x W x C\n",
    "#     return np.array(images)\n",
    "\n",
    "def old_flow_loader(path):\n",
    "    flow_x = []\n",
    "    flow_y = []\n",
    "    for img in os.listdir(path):\n",
    "        if img.endswith('x.png'):\n",
    "            flow_x.append(img)\n",
    "        else:\n",
    "            flow_y.append(img)\n",
    "    flow_x = sorted(flow_x)\n",
    "    flow_y = sorted(flow_y)\n",
    "    count = 0\n",
    "    required_frames = 64\n",
    "    frame_output = 5\n",
    "    length = len(flow_x)\n",
    "\n",
    "    flow_x_list = flow_x[::frame_output]\n",
    "    flow_y_list = flow_y[::frame_output]\n",
    "\n",
    "    missing_frames = required_frames - len(flow_x_list)\n",
    "    if(missing_frames>0):\n",
    "        remove_element = (missing_frames//frame_output)+1\n",
    "        del flow_x_list[-remove_element:]\n",
    "        del flow_y_list[-remove_element:]\n",
    "        missing_frames = required_frames - len(flow_x_list)\n",
    "        [flow_x_list.append(i) for i in flow_x[-missing_frames:]]\n",
    "        [flow_y_list.append(i) for i in flow_y[-missing_frames:]]\n",
    "    elif(missing_frames<0): #contains extra frames and need to remove\n",
    "        del flow_x_list[-abs(missing_frames):]\n",
    "        del flow_y_list[-abs(missing_frames):]\n",
    "    images = []    \n",
    "    for x,y in zip(flow_x_list,flow_y_list):\n",
    "        img_x = cv2.imread(path + x, cv2.IMREAD_GRAYSCALE)\n",
    "        img_x = preprocess_grayscale_image(img_x,(320,240))\n",
    "        img_y = cv2.imread(path + y, cv2.IMREAD_GRAYSCALE)\n",
    "        img_y = preprocess_grayscale_image(img_y,(320,240))\n",
    "        images.append(np.stack((img_x,img_y)).transpose([1,2,0])) #output shape T x H x W x C\n",
    "    return np.array(images)\n",
    "\"\"\"\n",
    "Make dataset with following input arguments:\n",
    "split_file : .txt file containing path to each modalities to be trained\n",
    "root: dataset folder path\n",
    "mode: decides how many samples we need in the output Ex: \"RGB\" or \"V\", \"FLOW\" or \"F\", \"VF\",\n",
    "        \"VFT\", \"VFTP\" (video, flow, tactile, position)\n",
    "\"\"\"\n",
    "def make_dataset(split_file, root, mode, transform):\n",
    "    dataset = []\n",
    "    file_path = np.loadtxt(split_file,dtype=str)\n",
    "    prefix = root.split(file_path[0][:32])[0]\n",
    "    file_names = ['front_rgb.mp4','left_rgb.mp4','tactile.txt','pos.txt','label.txt','flow']\n",
    "    for i,file in enumerate(file_path):\n",
    "        front_video_path = prefix + file + file_names[0]\n",
    "        left_video_path = prefix + file + file_names[1]\n",
    "        tactile_path = prefix +\"/Visual-Tactile_Dataset/tactile_data/\"+file.split(file[:32])[1] + file_names[2]\n",
    "        pos_path = prefix + file + file_names[3]\n",
    "        label_path = prefix + file + file_names[4]\n",
    "        front_flow_path = prefix + file + file_names[5] + '/' + 'front_rgb/'\n",
    "        left_flow_path = prefix + file + file_names[5] + '/' + 'left_rgb/'\n",
    "        dataset.append((front_video_path, left_video_path, tactile_path, pos_path, \\\n",
    "                       front_flow_path, left_flow_path, np.loadtxt(label_path)[3]))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "class VisualTactile(VisionDataset):\n",
    "    \"\"\"Dataloader customized to load Visual-Tactile dataset\n",
    "    Args:\n",
    "        split_file (string): path to .txt file where we have path to each test and train object.\n",
    "        root (string): Root directory path.\n",
    "        mode (string) : For future implementation. (Decide which data to load V, F, VF, VFT, VFTP)\n",
    "        transform (callable, optional): A function/transform that takes in\n",
    "            a sample and returns a transformed version.\n",
    "            E.g, ``transforms.RandomCrop`` for images.\n",
    "     Attributes:\n",
    "        samples (list): List of (sample path, class_index) tuples\n",
    "        targets (list): The class_index value for each image in the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split_file, root, mode=None, transform=None):\n",
    "        super(VisualTactile, self).__init__(root)\n",
    "\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.split_file = split_file\n",
    "        self.transform = transform\n",
    "        self.extensions = FILE_EXTENSIONS\n",
    "\n",
    "        # classes = name of the folder\n",
    "        # class_to_idx = dictionary with class name(folder name) and index\n",
    "        \"\"\"classes, class_to_idx = self._find_classes(self.root)\"\"\"\n",
    "        self.samples = make_dataset(self.split_file, self.root, self.mode, self.transform)\n",
    "#         self.samples = old_make_dataset(self.root, class_to_idx, extensions, is_valid_file)\n",
    "#         print(\"Samaples\" , self.samples[0])\n",
    "        if len(self.samples) == 0:\n",
    "            raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n",
    "                                \"Supported extensions are: \" + \",\".join(extensions)))\n",
    "\n",
    "        \"\"\"self.classes = classes\n",
    "        self.class_to_idx = class_to_idx\"\"\"\n",
    "        self.targets = [s[-1] for s in self.samples]\n",
    "\n",
    "    def _find_classes(self, dir):\n",
    "        \"\"\"\n",
    "        Finds the class folders in a dataset.\n",
    "        Args:\n",
    "            dir (string): Root directory path.\n",
    "        Returns:\n",
    "            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
    "        Ensures:\n",
    "            No class is a subdirectory of another.\n",
    "        \"\"\"\n",
    "        if sys.version_info >= (3, 5):\n",
    "            # Faster and available in Python 3.5 and above\n",
    "            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
    "        else:\n",
    "            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "        classes.sort()\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (f_video, l_video, tactile, pos, f_flow, l_flow, target) where target is successfull pickup or not.\n",
    "        \"\"\"\n",
    "        #add conditoin here wich will return only the requested mode. Ex. V, or VFTP\n",
    "        f_video, l_video, tactile, pos, f_flow, l_flow, target = self.samples[index]\n",
    "#         sample_f_video, selected_frames = vid_loader(f_video)\n",
    "#         sample_l_video = vid_loader(l_video)\n",
    "        sample_tactile = tactile_loader(tactile)\n",
    "#         sample_pos = tactile_loader(pos)\n",
    "        sample_target = target\n",
    "#         sample_f_flow = flow_loader(f_flow, selected_frames)\n",
    "#         sample_l_flow = flow_loader(l_flow)\n",
    "        if self.transform is not None:\n",
    "            sample_f_video = self.transform(sample_f_video)\n",
    "            sample_l_video = self.transform(sample_l_video)\n",
    "#         return sample_f_video, sample_l_video, sample_tactile, sample_pos, sample_target\n",
    "#         return video_to_tensor(sample_f_video), video_to_tensor(sample_l_video), \\\n",
    "#                 torch.from_numpy(sample_tactile), torch.from_numpy(sample_pos), \\\n",
    "#                 video_to_tensor(sample_f_flow), video_to_tensor(sample_l_flow), \\\n",
    "#                 torch.from_numpy(np.array(sample_target))\n",
    "#         return video_to_tensor(sample_f_video), video_to_tensor(sample_l_video), \\\n",
    "#                 torch.from_numpy(sample_tactile), torch.from_numpy(sample_pos), \\\n",
    "#                 torch.from_numpy(np.array(sample_target))\n",
    "        return sample_tactile, sample_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "class VisualTactileFolder(VisualTactile):\n",
    "    \"\"\"A generic data loader where the images are arranged in this way: ::\n",
    "        root/dog/xxx.png\n",
    "        root/dog/xxy.png\n",
    "        root/dog/xxz.png\n",
    "        root/cat/123.png\n",
    "        root/cat/nsdf3.png\n",
    "        root/cat/asd932_.png\n",
    "    Args:\n",
    "        root (string): Root directory path.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        loader (callable, optional): A function to load an image given its path.\n",
    "        is_valid_file (callable, optional): A function that takes path of an Image file\n",
    "            and check if the file is a valid_file (used to check of corrupt files)\n",
    "     Attributes:\n",
    "        classes (list): List of the class names.\n",
    "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
    "        imgs (list): List of (image path, class_index) tuples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split_file, root, mode=None, transform=None):\n",
    "        super(VisualTactileFolder, self).__init__(split_file, root, mode=None, transform=None)\n",
    "        self.imgs = self.samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root = '/media/pritesh/Entertainment/Visual-Tactile_Dataset/dataset/'\n",
    "ds = VisualTactile('../master_i3d/trainv2.txt', root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ds.samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,  88.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       ...,\n",
       "       [  0.,   0., 126., ...,   0., 221.,   0.],\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tactile = ds[0][0]\n",
    "ds[10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = 'frame_0000_x.png'\n",
    "int(s[6:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder = VisualTactileFolder('test.txt',root)\n",
    "tactile_path = folder.samples[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tactile_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = np.array(tactile[7]).reshape(4,-1)\n",
    "plt.imshow(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm = (sr - sr.mean())/np.std(sr)\n",
    "# plt.imshow(norm)\n",
    "np.linalg.norm(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fixing bug in flowloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vid_loader(path):\n",
    "    \"\"\"\n",
    "    1. Sample 64 frames from the video at equal interval.\n",
    "    2. Compress image from (640 x 480) to (320 x 240)\n",
    "    3.\n",
    "    3. Use Random crop to get (224 x 224)\n",
    "    Args:\n",
    "        dir (string): Root directory path.\n",
    "    Returns:\n",
    "        tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
    "    Ensures:\n",
    "        No class is a subdirectory of another.\n",
    "    \"\"\"\n",
    "    #print(path)\n",
    "    vidcap = cv2.VideoCapture(path)\n",
    "    success,image = vidcap.read()\n",
    "    count = 0\n",
    "    required_frames = 64\n",
    "    frame_output = 5\n",
    "    # 18x18 = 324  frames in total\n",
    "    length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     print(\"length\",length)\n",
    "    fps = int(vidcap.get(cv2.CAP_PROP_FPS))\n",
    "#     print(\"FPS\", fps )\n",
    "    duration = length / fps\n",
    "#     print(\"duration\",duration)\n",
    "\n",
    "    images = []\n",
    "    frames = []\n",
    "    while success:\n",
    "        if((count%5)==0 and (len(images)<required_frames)):\n",
    "            image = preprocess_image(image, (320,240))\n",
    "            images.append(image)\n",
    "            frames.append(count)\n",
    "        success,image = vidcap.read()\n",
    "        count += 1\n",
    "#     print(\"normal len : \" ,len(images))\n",
    "    missing_frames = required_frames - len(images)\n",
    "#     print(\"missing frames : \" ,missing_frames)\n",
    "    if(missing_frames):\n",
    "        remove_element = (missing_frames//frame_output)+1\n",
    "        del images[-remove_element:]\n",
    "        del frames[-remove_element:]\n",
    "        vidcap = cv2.VideoCapture(path)\n",
    "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, length-missing_frames-remove_element)\n",
    "        success,image = vidcap.read()\n",
    "        temp_count = 0\n",
    "        while success:\n",
    "            image = preprocess_image(image, (320,240))\n",
    "            images.append(image)\n",
    "            frames.append(length-missing_frames-remove_element+temp_count)\n",
    "            success,image = vidcap.read()\n",
    "            temp_count +=1\n",
    "    while(len(images) is not required_frames):\n",
    "        vidcap = cv2.VideoCapture(path)\n",
    "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, length-1)\n",
    "        success,image = vidcap.read()\n",
    "        image = preprocess_image(image, (320,240))\n",
    "        images.append(image)\n",
    "        frames.append(length-1)\n",
    "    return np.array(images),np.array(frames)\n",
    "\n",
    "def preprocess_grayscale_image(cv_frame, size):\n",
    "    \"\"\"\n",
    "    Args:   frame - cv image\n",
    "            size - tuple of (W x H)\n",
    "    \"\"\"\n",
    "    im = cv2.resize(cv_frame, size) #compress image\n",
    "    im = im/255 #normalize\n",
    "    im = (im - 0.5)/0.5\n",
    "    return im\n",
    "\n",
    "def flow_loader(path, frames):\n",
    "    flow_x = []\n",
    "    flow_y = []\n",
    "    images = []\n",
    "    images_x = []\n",
    "    images_y = []\n",
    "    count = 0\n",
    "    required_frames = 64\n",
    "    for img in os.listdir(path):\n",
    "        if img.endswith('x.png'):\n",
    "            flow_x.append(img)\n",
    "        else:\n",
    "            flow_y.append(img)\n",
    "    flow_x = sorted(flow_x)\n",
    "    flow_y = sorted(flow_y)\n",
    "    for frame in frames:\n",
    "        #bug fixed by count=frame\n",
    "        count = frame\n",
    "        while True:\n",
    "            if(frame==int(flow_x[count][6:10])):\n",
    "                images_x.append(flow_x[count])\n",
    "                images_y.append(flow_y[count])\n",
    "                break\n",
    "            else:\n",
    "                count += 1\n",
    "    print(images_x)\n",
    "    for x,y in zip(images_x,images_y):\n",
    "        img_x = cv2.imread(path + x, cv2.IMREAD_GRAYSCALE)\n",
    "        img_x = preprocess_grayscale_image(img_x,(320,240))\n",
    "        img_y = cv2.imread(path + y, cv2.IMREAD_GRAYSCALE)\n",
    "        img_y = preprocess_grayscale_image(img_y,(320,240))\n",
    "        images.append(np.stack((img_x,img_y)).transpose([1,2,0])) #output shape T x H x W x C\n",
    "    if(len(images) != required_frames):\n",
    "        raise ValueError(\"missing flow frames\")\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test your errorneous file here\n",
    "vpath = \"/media/pritesh/Entertainment/Visual-Tactile_Dataset/dataset/Coffeecup/50_470/top/7/front_rgb.mp4\"\n",
    "fpath = \"/home/pritesh/Desktop/front_rgb/\"\n",
    "images, frames = vid_loader(vpath)\n",
    "flow = flow_loader(fpath, frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(flow)\n",
    "# frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im = np.array(ds[0][0])\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tim = im.transpose([1,2,3,0])\n",
    "# normalized = (tim[0] - 0.5)/0.5\n",
    "plt.imshow(tim[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "x = np.array(Image.open('../../../../../frame_0052_x.png'))\n",
    "y = np.array(Image.open('../../../../../frame_0052_y.png'))\n",
    "flow = np.stack((x,y))\n",
    "nx = preprocess_grayscale_image(y, (320,240))\n",
    "plt.imshow(nx,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(y,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Max of tactile and normalize with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8592f6877132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_list_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmax_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds_train' is not defined"
     ]
    }
   ],
   "source": [
    "#finding maximum from the test set\n",
    "max_list = []\n",
    "for data in ds:\n",
    "    max_list.append(data[0].max())\n",
    "max_list_train = []\n",
    "for data in ds_train:\n",
    "    max_list.append(data[0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status 100.0 % completed % completeddd"
     ]
    }
   ],
   "source": [
    "datapt = len(ds.samples)\n",
    "max_tactile_test = np.arange(16)\n",
    "for i,data in enumerate(ds):\n",
    "    a = np.max(data[0],axis=0)\n",
    "    for j in range(16):\n",
    "        if(a[j]>max_tactile_test[j]):\n",
    "            max_tactile_test[j]=a[j]\n",
    "    sys.stdout.write(\"\\rstatus {} % completed\".format(((i+1)/datapt)*100))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status 100.0 % completed % completeddd"
     ]
    }
   ],
   "source": [
    "datapt = len(ds.samples)\n",
    "max_tactile_train = np.arange(16)\n",
    "for i,data in enumerate(ds):\n",
    "    a = np.max(data[0],axis=0)\n",
    "    for j in range(16):\n",
    "        if(a[j]>max_tactile_train[j]):\n",
    "            max_tactile_train[j]=a[j]\n",
    "    sys.stdout.write(\"\\rstatus {} % completed\".format(((i+1)/datapt)*100))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23655, 20662, 14496,  6475, 41133, 64793, 59317, 33177, 19897,\n",
       "       62084, 49874, 29170, 42944, 14976, 12311, 14331])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TACTILE_MAX_MAGNITUDE = np.array([23655, 20662, 14496,  6475, 41133, 64793, 59317, 33177, 19897,\n",
    "#        62084, 49874, 29170, 42944, 14976, 12311, 14331])\n",
    "max_tactile_test\n",
    "max_tactile_train\n",
    "TACTILE_MAX = np.max(np.stack((max_tactile_test,max_tactile_train)), axis=0)\n",
    "TACTILE_MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19844, 20007, 13876,  2212, 34104, 23571,  3778,  9039, 15113,\n",
       "       55713, 44345, 20870, 27789,  7034,  6753, 14331])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tactile_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23655, 20662, 14496,  6475, 41133, 64793, 59317, 33177, 19897,\n",
       "       62084, 49874, 29170, 42944, 14976, 12311, 14125])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tactile_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tactile/TACTILE_MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    sys.stdout.write(\"\\rDoing thing {}\".format(i))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max(max_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tactile image visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reshaped_tactile = tactile[:324,15].reshape(18,18)\n",
    "plt.imshow(reshaped_tactile)\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in range(16):\n",
    "    plt.plot(tactile[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tactile net v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class tactileNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(tactileNet,self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv3d(1, 16, kernel_size=(2,3,3), padding=1),nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Conv3d(16, 32, kernel_size=(2,3,3), padding=0),nn.ReLU(),nn.MaxPool3d(kernel_size=(2,2,2)))\n",
    "        self.fc1 = nn.Sequential(nn.Linear(1024, 64, bias=True), nn.ReLU(inplace=True), nn.Dropout(p = 0.5))\n",
    "        self.fc2 = nn.Linear(64,1)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "#        print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "#        print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "#        print(out.shape)\n",
    "        out = self.fc1(out)\n",
    "#        print(out.shape)\n",
    "        out = self.fc2(out)\n",
    "#        print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tactile net v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class tactileNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(tactileNet,self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv3d(1, 16, kernel_size=(2,3,3), padding=1),nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Conv3d(16, 32, kernel_size=(2,3,3), padding=0),nn.ReLU(),nn.MaxPool3d(kernel_size=(2,4,4)))\n",
    "        self.fc1 = nn.Sequential(nn.Linear(1024, 64, bias=True), nn.ReLU(inplace=True), nn.Dropout(p = 0.5))\n",
    "#         self.fc2 = nn.Linear(64,1)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "#         print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "#         print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "#         print(out.shape)\n",
    "        out = self.fc1(out)\n",
    "#         print(out.shape)\n",
    "        return out\n",
    "\n",
    "class tactileFusionNet(nn.Module):\n",
    "    def __init__(self, tac_model1, tac_model2, tac_model3, tac_model4):\n",
    "        super(tactileFusionNet,self).__init__()\n",
    "        self.tac_model1 = tac_model1\n",
    "        self.tac_model2 = tac_model2\n",
    "        self.tac_model3 = tac_model3\n",
    "        self.tac_model4 = tac_model4\n",
    "        self.fc1 = nn.Sequential(nn.Linear(64*4, 128, bias=True), nn.ReLU(inplace=True), nn.Dropout(p = 0.5))\n",
    "        self.fc2 = nn.Linear(128,1)\n",
    "    def forward(self, x1,x2,x3,x4):\n",
    "        t1 = self.tac_model1(x1)\n",
    "        print(t1.shape)\n",
    "        t2 = self.tac_model2(x2)\n",
    "        t3 = self.tac_model3(x3)\n",
    "        t4 = self.tac_model4(x4)\n",
    "        out = torch.cat((t1,t2,t3,t4),dim=1)\n",
    "        out = self.fc1(out)\n",
    "#         print(out.shape)\n",
    "        out = self.fc2(out)\n",
    "#         print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0612]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/media/pritesh/Entertainment/Visual-Tactile_Dataset/dataset/Coffeecup/50_843/back/0/tactile.txt\"\n",
    "a = tactile_loader(path)\n",
    "fuseNet1 = tactileNet()\n",
    "fuseNet2 = tactileNet()\n",
    "fuseNet3 = tactileNet()\n",
    "fuseNet4 = tactileNet()\n",
    "masterNet = tactileFusionNet(fuseNet1,fuseNet2,fuseNet3,fuseNet4)\n",
    "masterNet(a[0].unsqueeze(0).float(),a[1].unsqueeze(0).float(),a[2].unsqueeze(0).float(),a[3].unsqueeze(0).float())\n",
    "# inp = a[0].unsqueeze(0)\n",
    "# fuseNet(inp.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorboardX\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from charades_dataset import Charades as Dataset\n",
    "from visual_tactile_dataset import VisualTactile as Dataset\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class tactileNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(tactileNet,self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv3d(1, 16, kernel_size=(2,3,3), padding=1),nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Conv3d(16, 32, kernel_size=(2,3,3), padding=0),nn.ReLU(),nn.MaxPool3d(kernel_size=(2,2,2)))\n",
    "        self.fc1 = nn.Sequential(nn.Linear(1024, 64, bias=True), nn.ReLU(inplace=True), nn.Dropout(p = 0.5))\n",
    "        self.fc2 = nn.Linear(64,1)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "#        print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "#        print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "#        print(out.shape)\n",
    "        out = self.fc1(out)\n",
    "#        print(out.shape)\n",
    "        out = self.fc2(out)\n",
    "#        print(out.shape)\n",
    "        return out\n",
    "\n",
    "net = tactileNet()\n",
    "net = nn.DataParallel(net)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.0000001)\n",
    "lr_sched = optim.lr_scheduler.MultiStepLR(optimizer, [50, 100, 150, 200])\n",
    "data = torch.load('model_31.tar',map_location='cpu')\n",
    "net.load_state_dict(data['model_state'])\n",
    "optimizer.load_state_dict(data['optimizer_state'])\n",
    "lr_sched.load_state_dict(data['scheduler_state'])\n",
    "\n",
    "def run(init_lr=0.001, max_steps=200, mode='rgb', root='/media/pritesh/Entertainment/Visual-Tactile_Dataset/dataset/',\\\n",
    "        train_split='train.txt', test_split='test.txt', batch_size=1, save_model=''):\n",
    "    writer = tensorboardX.SummaryWriter()\n",
    "    # setup dataset\n",
    "\n",
    "    dataset = Dataset(train_split, root, mode)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    val_dataset = Dataset(test_split, root, mode)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    dataloaders = {'train': dataloader, 'val': val_dataloader}\n",
    "    datasets = {'train': dataset, 'val': val_dataset}\n",
    "    \n",
    "    steps = 0\n",
    "    with open('i3d_video.txt', 'w') as file: \n",
    "        file.write(\"train and validation loss file\\n\")\n",
    "    # train it\n",
    "    print ('Step {}/{}'.format(steps, max_steps))\n",
    "    print ('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    net.train(False)  # Set model to evaluate mode\n",
    "\n",
    "    tot_cls_loss = 0.0\n",
    "    num_iter = 0\n",
    "    count = 0\n",
    "    optimizer.zero_grad()\n",
    "    # Iterate over data.\n",
    "    phase = 'val'\n",
    "    for data in dataloaders[phase]:\n",
    "        num_iter += 1\n",
    "        # get the inputs\n",
    "        tactile, labels = data\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            tact_input = Variable(tactile.cuda())\n",
    "            t = tact_input.size(2)\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            tact_input = Variable(tactile)\n",
    "            t = tact_input.size(2)\n",
    "            labels = Variable(labels)\n",
    "\n",
    "        out = net(tact_input.float())\n",
    "        #print('prediction output = ', per_frame_logits.shape)\n",
    "        #print('labels = ',labels.shape)\n",
    "        # compute classification loss (with max-pooling along time B x C x T)\n",
    "        out = out.squeeze(1)\n",
    "        cls_loss = F.binary_cross_entropy_with_logits(out.double(), labels.double())\n",
    "#         print(cls_loss,labels)\n",
    "        tot_cls_loss += cls_loss.item()\n",
    "#         cls_loss.backward()\n",
    "        print('{}. {} Loss: {:.4f} and lr: {}'.format(num_iter, phase,tot_cls_loss/num_iter,init_lr))\n",
    "        with open('i3d_video.txt', 'a') as file: \n",
    "            file.write(\"%f\\n\" %(tot_cls_loss/num_iter))\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "        if phase == 'val':\n",
    "            writer.add_scalar('error/'+ phase , (tot_cls_loss/num_iter), num_iter)\n",
    "        else:\n",
    "            writer.add_scalar('error/'+ phase, (tot_cls_loss/num_iter), num_iter)\n",
    "    #save error at every epoch\n",
    "    writer.add_scalar('errorAtEpoch/'+phase, (tot_cls_loss/num_iter), steps)\n",
    "    tot_cls_loss = 0.\n",
    "#if(steps%50 == 0):\n",
    "#    torch.save(fusedNet.module.state_dict(), save_model+phase+str(steps).zfill(6)+'.pt')\n",
    "#    save_checkpoint(fusedNet, optimizer, lr_sched, steps)\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = np.where(a<1)\n",
    "a[index] = 0\n",
    "np.max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tactile[0]\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "time = np.linspace(0,24,len(a))\n",
    "for i in range(1,17):\n",
    "    plt.plot(time,a[:,i-1],label=(i-1))\n",
    "    plt.legend()\n",
    "#     plt.axvline(label[0]/10, color=\"green\")\n",
    "#     plt.axvline(label[1]/10, color=\"yellow\")\n",
    "#     plt.axvline(label[2]/10, color=\"r\")\n",
    "    plt.title(\"Cola 150-26 right-8\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(15,7))\n",
    "for i in range(1,17):\n",
    "    plt.plot(np.linspace(0,24,len(td)),td[:,i-1],label=(i-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class tactileNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(tactileNet,self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv3d(1, 16, kernel_size=(2,3,3), padding=1),nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Conv3d(16, 32, kernel_size=(2,3,3), padding=0),nn.ReLU(),nn.MaxPool3d(kernel_size=(2,2,2)))\n",
    "        self.fc1 = nn.Sequential(nn.Linear(1024, 64, bias=True), nn.ReLU(inplace=True), nn.Dropout(p = 0.5))\n",
    "        self.fc2 = nn.Linear(64,1)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "        print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        print(out.shape)\n",
    "        out = self.fc1(out)\n",
    "        print(out.shape)\n",
    "        out = self.fc2(out)\n",
    "        print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = tactileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tactile[0][4,:].reshape(4,4,1)\n",
    "torch.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_input(data):\n",
    "    output = []\n",
    "    frames = np.random.randint(0,431,64)\n",
    "    for frame in frames:\n",
    "        output.append(data[frame,:].reshape(4,4,1))\n",
    "    return np.array(output)\n",
    "    \n",
    "inp = get_input(tactile[0])\n",
    "inp = video_to_tensor(inp)\n",
    "inp = inp.unsqueeze(0)\n",
    "inp.shape\n",
    "# inp = torch.from_numpy(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = net(inp.float())\n",
    "torch.save(net.state_dict(), 'ok.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "y = np.array(range(400))\n",
    "x = np.array(range(16))\n",
    "a = td\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "f = interpolate.interp2d(x, y, a, kind='linear')\n",
    "\n",
    "# xnew = np.linspace(0, 16, 4)\n",
    "# ynew = np.linspace(0, 24*18, 4)\n",
    "xnew = np.arange(16)\n",
    "ynew = np.linspace(0, 400, (24*18))\n",
    "znew = f(xnew, ynew)\n",
    "plt.figure(figsize=(15,8))\n",
    "for i in range(1,17):\n",
    "    plt.semilogy(np.linspace(0,24,len(td)),znew[:400,i-1],label=(i-1))\n",
    "    plt.legend()\n",
    "for i in range(1,17):\n",
    "    plt.semilogy(np.linspace(0,24,len(td)),td[:,i-1],label=(i-1))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# for i in range(len(tactile[0])):\n",
    "# #     plt.imshow(tactile[0][i,:].reshape(4,4),cmap='gray')\n",
    "#     cv2.imshow('im',tactile[0][i,:].reshape(4,4))\n",
    "#     cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optim.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.arange(16)\n",
    "a = np.max(tactile,axis=0)\n",
    "for i in range(16):\n",
    "    if(a[i]>b[i]):\n",
    "        b[i]=a[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "TACTILE_MAX_MAGNITUDE = np.array([23655, 20662, 14496,  6475, 41133, 64793, 59317, 33177, 19897,\n",
    "       62084, 49874, 29170, 42944, 14976, 12311, 14331])\n",
    "def tactile_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    output =[]\n",
    "    groups = []\n",
    "    tactile_frame = np.loadtxt(path)\n",
    "    tactile = tactile_frame.astype('float')\n",
    "    tactile = tactile/TACTILE_MAX_MAGNITUDE\n",
    "    #group of 4 sensors\n",
    "    n=4\n",
    "    out = [tactile[:,k:k+n] for k in range(0, tactile.shape[1], n)]\n",
    "    for data in out:\n",
    "        for i in range(data.shape[1]):\n",
    "            groups.append(tactile[:324,i].reshape(18,18,1))\n",
    "        temp = video_to_tensor(np.array(groups))\n",
    "        output.append(temp)\n",
    "        groups = []\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class tactileNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(tactileNet,self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv3d(1, 16, kernel_size=(2,3,3), padding=1),nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Conv3d(16, 32, kernel_size=(2,3,3), padding=0),nn.ReLU(),nn.MaxPool3d(kernel_size=(2,4,4)))\n",
    "        self.fc1 = nn.Sequential(nn.Linear(1024, 64, bias=True), nn.ReLU(inplace=True), nn.Dropout(p = 0.5))\n",
    "#         self.fc2 = nn.Linear(64,1)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "#         print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "#         print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "#         print(out.shape)\n",
    "        out = self.fc1(out)\n",
    "#         print(out.shape)\n",
    "        return out\n",
    "\n",
    "class tactileFusionNet(nn.Module):\n",
    "    def __init__(self, tac_model1, tac_model2, tac_model3, tac_model4):\n",
    "        super(tactileNet,self).__init__()\n",
    "        self.tac_model1 = tac_model1\n",
    "        self.tac_model2 = tac_model2\n",
    "        self.tac_model3 = tac_model3\n",
    "        self.tac_model4 = tac_model4\n",
    "        self.fc1 = nn.Sequential(nn.Linear(64*4, 128, bias=True), nn.ReLU(inplace=True), nn.Dropout(p = 0.5))\n",
    "        self.fc2 = nn.Linear(128,1)\n",
    "    def forward(self, x1,x2,x3,x4):\n",
    "        t1 = self.tac_model1(x1)\n",
    "        t2 = self.tac_model2(x2)\n",
    "        t3 = self.tac_model3(x3)\n",
    "        t4 = self.tac_model4(x4)\n",
    "        out = torch.cat((t1,t2,t3,t4),dim=1)\n",
    "        out = self.fc1(out)\n",
    "#         print(out.shape)\n",
    "        out = self.fc2(out)\n",
    "#         print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 5, 18, 18])\n",
      "torch.Size([1, 32, 2, 4, 4])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0734]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/media/pritesh/Entertainment/Visual-Tactile_Dataset/dataset/Coffeecup/50_843/back/0/tactile.txt\"\n",
    "a = tactile_loader(path)\n",
    "fuseNet = tactileNet()\n",
    "inp = a[0].unsqueeze(0)\n",
    "fuseNet(inp.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "* https://stackoverflow.com/questions/51420923/resampling-a-signal-with-scipy-signal-resample\n",
    "* Why normalization is required in neural network: https://visualstudiomagazine.com/articles/2014/01/01/how-to-standardize-data-for-neural-networks.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
