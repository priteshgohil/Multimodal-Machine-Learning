{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "if('/opt/ros/kinetic/lib/python2.7/dist-packages' in sys.path):\n",
    "    sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# base class for dataloader vision.py\n",
    "\n",
    "class VisionDataset(data.Dataset):\n",
    "    _repr_indent = 4\n",
    "\n",
    "    def __init__(self, root, transforms=None, transform=None, target_transform=None):\n",
    "        if isinstance(root, torch._six.string_classes):\n",
    "            root = os.path.expanduser(root)\n",
    "        self.root = root\n",
    "\n",
    "        has_transforms = transforms is not None\n",
    "        has_separate_transform = transform is not None or target_transform is not None\n",
    "        if has_transforms and has_separate_transform:\n",
    "            raise ValueError(\"Only transforms or transform/target_transform can \"\n",
    "                             \"be passed as argument\")\n",
    "\n",
    "        # for backwards-compatibility\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        if has_separate_transform:\n",
    "            transforms = StandardTransform(transform, target_transform)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __repr__(self):\n",
    "        head = \"Dataset \" + self.__class__.__name__\n",
    "        body = [\"Number of datapoints: {}\".format(self.__len__())]\n",
    "        if self.root is not None:\n",
    "            body.append(\"Root location: {}\".format(self.root))\n",
    "        body += self.extra_repr().splitlines()\n",
    "        if hasattr(self, \"transforms\") and self.transforms is not None:\n",
    "            body += [repr(self.transforms)]\n",
    "        lines = [head] + [\" \" * self._repr_indent + line for line in body]\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def _format_transform_repr(self, transform, head):\n",
    "        lines = transform.__repr__().splitlines()\n",
    "        return ([\"{}{}\".format(head, lines[0])] +\n",
    "                [\"{}{}\".format(\" \" * len(head), line) for line in lines[1:]])\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "class StandardTransform(object):\n",
    "    def __init__(self, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __call__(self, input, target):\n",
    "        if self.transform is not None:\n",
    "            input = self.transform(input)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return input, target\n",
    "\n",
    "    def _format_transform_repr(self, transform, head):\n",
    "        lines = transform.__repr__().splitlines()\n",
    "        return ([\"{}{}\".format(head, lines[0])] +\n",
    "                [\"{}{}\".format(\" \" * len(head), line) for line in lines[1:]])\n",
    "\n",
    "    def __repr__(self):\n",
    "        body = [self.__class__.__name__]\n",
    "        if self.transform is not None:\n",
    "            body += self._format_transform_repr(self.transform,\n",
    "                                                \"Transform: \")\n",
    "        if self.target_transform is not None:\n",
    "            body += self._format_transform_repr(self.target_transform,\n",
    "                                                \"Target transform: \")\n",
    "\n",
    "        return '\\n'.join(body)\n",
    "\n",
    "\n",
    "FILE_EXTENSIONS = ('front_0.jpg','tactile.txt','label.txt','front_rgb.mp4','left_rgb.mp4', 'pos.txt')\n",
    "\n",
    "def get_params(img, output_size):\n",
    "    \"\"\"random crop input sequence of frames.\n",
    "    Args:\n",
    "        img (PIL Image): Image to be cropped.\n",
    "        output_size (tuple): Expected output size of the crop.\n",
    "    Returns:\n",
    "        tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n",
    "    \"\"\"\n",
    "    t, h, w, c = img.shape\n",
    "    th, tw = output_size\n",
    "    if w == tw and h == th:\n",
    "        return 0, 0, h, w\n",
    "\n",
    "    i = random.randint(0, h - th) if h!=th else 0\n",
    "    j = random.randint(0, w - tw) if w!=tw else 0\n",
    "    return i, j, th, tw\n",
    "\n",
    "def random_crop(imgs):\n",
    "    i, j, h, w = get_params(a, (224,224))\n",
    "    imgs = imgs[:, i:i+h, j:j+w, :]\n",
    "    return imgs\n",
    "\n",
    "def tactile_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    up_samples=24*18\n",
    "    with open(path, 'rb') as f:\n",
    "        tactile_frame = pd.read_csv(f,delimiter=' ', header=None)\n",
    "        tactile = tactile_frame.as_matrix()\n",
    "        tactile = tactile.astype('float')\n",
    "        tactile = signal.resample(tactile,up_samples)\n",
    "        return tactile #to get samples equal to the video frames, make length variable in vid_loader global and\n",
    "                        #then tactile[:lenth]\n",
    "\n",
    "def video_to_tensor(frames):\n",
    "    \"\"\"Convert a ``numpy.ndarray`` to tensor.\n",
    "    Converts a numpy.ndarray (T x H x W x C)\n",
    "    to a torch.FloatTensor of shape (C x T x H x W)\n",
    "\n",
    "    Args:\n",
    "         pic (numpy.ndarray): Video to be converted to tensor.\n",
    "    Returns:\n",
    "         Tensor: Converted video.\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(frames.transpose([3,0,1,2]))\n",
    "\n",
    "def compress_image(cv_frame, size):\n",
    "    \"\"\"\n",
    "    Args:   frame - cv image\n",
    "            size - tuple of (W x H)\n",
    "    \"\"\"\n",
    "    return cv2.resize(cv_frame, size)\n",
    "\n",
    "def preprocess_image(cv_frame, size):\n",
    "    \"\"\"\n",
    "    Args:   frame - cv image\n",
    "            size - tuple of (W x H)\n",
    "    \"\"\"\n",
    "    im = cv2.resize(cv_frame, size) #compress image\n",
    "    im = np.array(Image.fromarray(cv2.cvtColor(im,cv2.COLOR_BGR2RGB))) #convert to RGB image\n",
    "    im = im/255 #normalize\n",
    "    return im\n",
    "    \n",
    "def vid_loader(path):\n",
    "    \"\"\"\n",
    "    1. Sample 64 frames from the video at equal interval.\n",
    "    2. Compress image from (640 x 480) to (320 x 240)\n",
    "    3.\n",
    "    3. Use Random crop to get (224 x 224)\n",
    "    Args:\n",
    "        dir (string): Root directory path.\n",
    "    Returns:\n",
    "        tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
    "    Ensures:\n",
    "        No class is a subdirectory of another.\n",
    "    \"\"\"\n",
    "    print(path)\n",
    "    vidcap = cv2.VideoCapture(path)\n",
    "    success,image = vidcap.read()\n",
    "    count = 0\n",
    "    required_frames = 64\n",
    "    frame_output = 5\n",
    "    # 18x18 = 324  frames in total\n",
    "    length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     print(\"length\",length)\n",
    "    fps = int(vidcap.get(cv2.CAP_PROP_FPS))\n",
    "#     print(\"FPS\", fps )\n",
    "    duration = length / fps\n",
    "#     print(\"duration\",duration)\n",
    "\n",
    "    images = []\n",
    "    while success:\n",
    "        if((count%5)==0 and (len(images)<required_frames)):\n",
    "            image = preprocess_image(image, (320,240))\n",
    "            images.append(image)\n",
    "        success,image = vidcap.read()\n",
    "        count += 1\n",
    "#     print(\"normal len : \" ,len(images))\n",
    "    missing_frames = required_frames - len(images)\n",
    "#     print(\"missing frames : \" ,missing_frames)\n",
    "    if(missing_frames):\n",
    "        remove_element = (missing_frames//frame_output)+1\n",
    "        del images[-remove_element:]\n",
    "        vidcap = cv2.VideoCapture(path)\n",
    "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, length-missing_frames-remove_element)\n",
    "        success,image = vidcap.read()\n",
    "        while success:\n",
    "            image = preprocess_image(image, (320,240))\n",
    "            images.append(image)\n",
    "            success,image = vidcap.read()\n",
    "    while(len(images) is not required_frames):\n",
    "        vidcap = cv2.VideoCapture(path)\n",
    "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, length-1)\n",
    "        success,image = vidcap.read()\n",
    "        image = preprocess_image(image, (320,240))\n",
    "        images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "def preprocess_grayscale_image(cv_frame, size):\n",
    "    \"\"\"\n",
    "    Args:   frame - cv image\n",
    "            size - tuple of (W x H)\n",
    "    \"\"\"\n",
    "    im = cv2.resize(cv_frame, size) #compress image\n",
    "    im = im/255 #normalize\n",
    "    return im\n",
    "\n",
    "def flow_loader(path):\n",
    "    flow_x = []\n",
    "    flow_y = []\n",
    "    for img in os.listdir(path):\n",
    "        if img.endswith('x.png'):\n",
    "            flow_x.append(img)\n",
    "        else:\n",
    "            flow_y.append(img)\n",
    "    flow_x = sorted(flow_x)\n",
    "    flow_y = sorted(flow_y)\n",
    "    count = 0\n",
    "    required_frames = 64\n",
    "    frame_output = 5\n",
    "    length = len(flow_x)\n",
    "\n",
    "    flow_x_list = flow_x[::frame_output]\n",
    "    flow_y_list = flow_y[::frame_output]\n",
    "\n",
    "    missing_frames = required_frames - len(flow_x_list)\n",
    "    if(missing_frames):\n",
    "        remove_element = (missing_frames//frame_output)+1\n",
    "        del flow_x_list[-remove_element:]\n",
    "        del flow_y_list[-remove_element:]\n",
    "    missing_frames = required_frames - len(flow_x_list)\n",
    "    [flow_x_list.append(i) for i in flow_x[-missing_frames:]]\n",
    "    [flow_y_list.append(i) for i in flow_y[-missing_frames:]]\n",
    "\n",
    "    images = []    \n",
    "    for x,y in zip(flow_x_list,flow_y_list):\n",
    "        img_x = cv2.imread(path + x, cv2.IMREAD_GRAYSCALE)\n",
    "        img_x = preprocess_grayscale_image(img_x,(320,240))\n",
    "        img_y = cv2.imread(path + y, cv2.IMREAD_GRAYSCALE)\n",
    "        img_y = preprocess_grayscale_image(img_y,(320,240))\n",
    "        images.append(np.stack((img_x,img_y)).transpose([1,2,0]))\n",
    "    return np.array(images)\n",
    "\"\"\"\n",
    "Make dataset with following input arguments:\n",
    "split_file : .txt file containing path to each modalities to be trained\n",
    "root: dataset folder path\n",
    "mode: decides how many samples we need in the output Ex: \"RGB\" or \"V\", \"FLOW\" or \"F\", \"VF\",\n",
    "        \"VFT\", \"VFTP\" (video, flow, tactile, position)\n",
    "\"\"\"\n",
    "def make_dataset(split_file, root, mode, transform):\n",
    "    dataset = []\n",
    "    file_path = np.loadtxt(split_file,dtype=str)\n",
    "    prefix = root.split(file_path[0][:32])[0]\n",
    "    file_names = ['front_rgb.mp4','left_rgb.mp4','tactile.txt','pos.txt','label.txt','flow']\n",
    "#     print(prefix+file_path[0]+file_names[0])\n",
    "    for i,file in enumerate(file_path):\n",
    "        front_video_path = prefix + file + file_names[0]\n",
    "        left_video_path = prefix + file + file_names[1]\n",
    "        tactile_path = prefix + file + file_names[2]\n",
    "        pos_path = prefix + file + file_names[3]\n",
    "        label_path = prefix + file + file_names[4]\n",
    "        front_flow_path = prefix + file + file_names[5] + '/' + 'front_rgb/'\n",
    "        left_flow_path = prefix + file + file_names[5] + '/' + 'left_rgb/'\n",
    "        dataset.append((front_video_path, left_video_path, tactile_path, pos_path, \\\n",
    "                       front_flow_path, left_flow_path, np.loadtxt(label_path)[3]))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "class VisualTactile(VisionDataset):\n",
    "    \"\"\"Dataloader customized to load Visual-Tactile dataset\n",
    "    Args:\n",
    "        split_file (string): path to .txt file where we have path to each test and train object.\n",
    "        root (string): Root directory path.\n",
    "        mode (string) : For future implementation. (Decide which data to load V, F, VF, VFT, VFTP)\n",
    "        transform (callable, optional): A function/transform that takes in\n",
    "            a sample and returns a transformed version.\n",
    "            E.g, ``transforms.RandomCrop`` for images.\n",
    "     Attributes:\n",
    "        samples (list): List of (sample path, class_index) tuples\n",
    "        targets (list): The class_index value for each image in the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split_file, root, mode=None, transform=None):\n",
    "        super(VisualTactile, self).__init__(root)\n",
    "\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.split_file = split_file\n",
    "        self.transform = transform\n",
    "        self.extensions = FILE_EXTENSIONS\n",
    "\n",
    "        # classes = name of the folder\n",
    "        # class_to_idx = dictionary with class name(folder name) and index\n",
    "        \"\"\"classes, class_to_idx = self._find_classes(self.root)\"\"\"\n",
    "        self.samples = make_dataset(self.split_file, self.root, self.mode, self.transform)\n",
    "#         self.samples = old_make_dataset(self.root, class_to_idx, extensions, is_valid_file)\n",
    "#         print(\"Samaples\" , self.samples[0])\n",
    "        if len(self.samples) == 0:\n",
    "            raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n",
    "                                \"Supported extensions are: \" + \",\".join(extensions)))\n",
    "\n",
    "        \"\"\"self.classes = classes\n",
    "        self.class_to_idx = class_to_idx\"\"\"\n",
    "        self.targets = [s[-1] for s in self.samples]\n",
    "\n",
    "    def _find_classes(self, dir):\n",
    "        \"\"\"\n",
    "        Finds the class folders in a dataset.\n",
    "        Args:\n",
    "            dir (string): Root directory path.\n",
    "        Returns:\n",
    "            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
    "        Ensures:\n",
    "            No class is a subdirectory of another.\n",
    "        \"\"\"\n",
    "        if sys.version_info >= (3, 5):\n",
    "            # Faster and available in Python 3.5 and above\n",
    "            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
    "        else:\n",
    "            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "        classes.sort()\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (f_video, l_video, tactile, pos, f_flow, l_flow, target) where target is successfull pickup or not.\n",
    "        \"\"\"\n",
    "        #add conditoin here wich will return only the requested mode. Ex. V, or VFTP\n",
    "        f_video, l_video, tactile, pos, f_flow, l_flow, target = self.samples[index]\n",
    "        sample_f_video = vid_loader(f_video)\n",
    "        sample_l_video = vid_loader(l_video)\n",
    "        sample_tactile = tactile_loader(tactile)\n",
    "        sample_pos = tactile_loader(pos)\n",
    "        sample_target = target\n",
    "        sample_f_flow = flow_loader(f_flow)\n",
    "        sample_l_flow = flow_loader(l_flow)\n",
    "        if self.transform is not None:\n",
    "            sample_f_video = self.transform(sample_f_video)\n",
    "            sample_l_video = self.transform(sample_l_video)\n",
    "#         return sample_f_video, sample_l_video, sample_tactile, sample_pos, sample_target\n",
    "        return video_to_tensor(sample_f_video), video_to_tensor(sample_l_video),torch.from_numpy(sample_tactile), torch.from_numpy(sample_pos), torch.from_numpy(np.array(sample_target))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "class VisualTactileFolder(VisualTactile):\n",
    "    \"\"\"A generic data loader where the images are arranged in this way: ::\n",
    "        root/dog/xxx.png\n",
    "        root/dog/xxy.png\n",
    "        root/dog/xxz.png\n",
    "        root/cat/123.png\n",
    "        root/cat/nsdf3.png\n",
    "        root/cat/asd932_.png\n",
    "    Args:\n",
    "        root (string): Root directory path.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        loader (callable, optional): A function to load an image given its path.\n",
    "        is_valid_file (callable, optional): A function that takes path of an Image file\n",
    "            and check if the file is a valid_file (used to check of corrupt files)\n",
    "     Attributes:\n",
    "        classes (list): List of the class names.\n",
    "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
    "        imgs (list): List of (image path, class_index) tuples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split_file, root, mode=None, transform=None):\n",
    "        super(VisualTactileFolder, self).__init__(split_file, root, mode=None, transform=None)\n",
    "        self.imgs = self.samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/media/pritesh/Entertainment/Visual-Tactile_Dataset/dataset/Cheez/100_432/right/0/'\n",
    "ds = VisualTactile('test.txt', root)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/pritesh/Entertainment/Visual-Tactile_Dataset/dataset/Coffeecup/50_843/top/3/front_rgb.mp4\n",
      "/media/pritesh/Entertainment/Visual-Tactile_Dataset/dataset/Coffeecup/50_843/top/3/left_rgb.mp4\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "x = torchvision.utils.make_grid(ds[0][0])\n",
    "writer.add_image(\"images/obj1\", x, 0)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = VisualTactileFolder('test.txt', root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/pritesh/Entertainment/Visual-Tactile_Dataset/dataset/Coffeecup/50_843/top/3/flow/front_rgb/'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.samples[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = df.samples[0][4]\n",
    "def flow_loader(path):\n",
    "    flow_x = []\n",
    "    flow_y = []\n",
    "    for img in os.listdir(path):\n",
    "        if img.endswith('x.png'):\n",
    "            flow_x.append(img)\n",
    "        else:\n",
    "            flow_y.append(img)\n",
    "    flow_x = sorted(flow_x)\n",
    "    flow_y = sorted(flow_y)\n",
    "    count = 0\n",
    "    required_frames = 64\n",
    "    frame_output = 5\n",
    "    # # 18x18 = 324  frames in total\n",
    "    length = len(flow_x)\n",
    "\n",
    "    flow_x_list = flow_x[::frame_output]\n",
    "    flow_y_list = flow_y[::frame_output]\n",
    "\n",
    "    missing_frames = required_frames - len(flow_x_list)\n",
    "    if(missing_frames):\n",
    "        remove_element = (missing_frames//frame_output)+1\n",
    "        del flow_x_list[-remove_element:]\n",
    "        del flow_y_list[-remove_element:]\n",
    "    missing_frames = required_frames - len(flow_x_list)\n",
    "    [flow_x_list.append(i) for i in flow_x[-missing_frames:]]\n",
    "    [flow_y_list.append(i) for i in flow_y[-missing_frames:]]\n",
    "\n",
    "    images = []    \n",
    "    for x,y in zip(flow_x_list,flow_y_list):\n",
    "        img_x = cv2.imread(path + x, cv2.IMREAD_GRAYSCALE)\n",
    "        img_x = preprocess_grayscale_image(img_x,(320,240))\n",
    "        img_y = cv2.imread(path + y, cv2.IMREAD_GRAYSCALE)\n",
    "        img_y = preprocess_grayscale_image(img_y,(320,240))\n",
    "        images.append(np.stack((img_x,img_y)).transpose([1,2,0]))\n",
    "    return np.array(images)\n",
    "\n",
    "# images = []\n",
    "# while (len(images)<required_frames):\n",
    "#     if((count%5)==0):\n",
    "#         image = preprocess_image(image, (320,240))\n",
    "#         images.append(image)\n",
    "#     count += frame_output\n",
    "# #     print(\"normal len : \" ,len(images))\n",
    "# missing_frames = required_frames - len(images)\n",
    "# #     print(\"missing frames : \" ,missing_frames)\n",
    "# if(missing_frames):\n",
    "#     remove_element = (missing_frames//frame_output)+1\n",
    "#     del images[-remove_element:]\n",
    "#     vidcap = cv2.VideoCapture(path)\n",
    "#     vidcap.set(cv2.CAP_PROP_POS_FRAMES, length-missing_frames-remove_element)\n",
    "#     success,image = vidcap.read()\n",
    "#     while success:\n",
    "#         image = preprocess_image(image, (320,240))\n",
    "#         images.append(image)\n",
    "#         success,image = vidcap.read()\n",
    "# while(len(images) is not required_frames):\n",
    "#     vidcap = cv2.VideoCapture(path)\n",
    "#     vidcap.set(cv2.CAP_PROP_POS_FRAMES, length-1)\n",
    "#     success,image = vidcap.read()\n",
    "#     image = preprocess_image(image, (320,240))\n",
    "#     images.append(image)\n",
    "# return np.array(images)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 240, 320, 2)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "out = flow_loader(path)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6,7,8,3,3,3,3311,423,5345,36,3]\n",
    "b = [9,9,9,9,9,9,9,9,9,9,9,9,9]\n",
    "for a,b in zip(a,b):\n",
    "    print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(cv_frame, size):\n",
    "    \"\"\"\n",
    "    Args:   frame - cv image\n",
    "            size - tuple of (W x H)\n",
    "    \"\"\"\n",
    "    im = cv2.resize(cv_frame, size) #compress image\n",
    "#     im = np.array(Image.fromarray(cv2.cvtColor(im,cv2.COLOR_BGR2RGB))) #convert to RGB image\n",
    "    im = im/255 #normalize\n",
    "    return im\n",
    "a = cv2.imread(path + 'frame_0205_x.png', cv2.IMREAD_GRAYSCALE)\n",
    "a = preprocess_image(a,(320,240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 320, 2)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose([1,2,0]).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"im\",a)\n",
    "cv2.waitKey(2000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboardX\n",
    "writer = tensorboardX.SummaryWriter()\n",
    "writer.add_scalar('stemp',1100,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_scalar('stemp',2500,1)\n",
    "writer.add_scalar('stemp',500,2)\n",
    "writer.add_scalar('stemp',1333,3)\n",
    "writer.add_scalar('stemp',1000,4)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
