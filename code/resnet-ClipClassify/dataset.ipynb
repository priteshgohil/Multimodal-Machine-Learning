{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "if('/opt/ros/kinetic/lib/python2.7/dist-packages' in sys.path):\n",
    "    sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "import cv2\n",
    "\n",
    "\n",
    "def video_to_tensor(frames):\n",
    "    \"\"\"Convert a ``numpy.ndarray`` to tensor.\n",
    "    Converts a numpy.ndarray (T x H x W x C)\n",
    "    to a torch.FloatTensor of shape (C x T x H x W)\n",
    "\n",
    "    Args:\n",
    "         pic (numpy.ndarray): Video to be converted to tensor.\n",
    "    Returns:\n",
    "         Tensor: Converted video.\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(frames.transpose([3,0,1,2]))\n",
    "\n",
    "def preprocess_image(cv_frame, size):\n",
    "    \"\"\"\n",
    "    Args:   frame - cv image\n",
    "            size - tuple of (W x H)\n",
    "    \"\"\"\n",
    "    im = cv2.resize(cv_frame, size) #compress image\n",
    "    im = np.array(Image.fromarray(cv2.cvtColor(im,cv2.COLOR_BGR2RGB))) #convert to RGB image\n",
    "    im = im/255 #normalize\n",
    "    im = (im-0.5)/0.5\n",
    "    return im\n",
    "\n",
    "def video_loader(vid_path, frames):\n",
    "    \"\"\"\n",
    "    Args:   vid_path - location of saved video frames\n",
    "            frames - numpy array of frame number to read\n",
    "    \"\"\"\n",
    "    resize_W = 160\n",
    "    resize_H = 120\n",
    "    images = []\n",
    "    for frame in frames:\n",
    "        image_name = \"frame_{:04n}.jpg\".format(frame)\n",
    "        img = cv2.imread(vid_path + image_name)\n",
    "        img = preprocess_image(img, (resize_W,resize_H))\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "def split_frames(start,stop,length):\n",
    "    \"\"\"\n",
    "    To match video frames with tactile,\n",
    "    Input:  start - starting frame of video\n",
    "            stop - last frame of video\n",
    "    output: sequence of frames in batch of length.\n",
    "    for ex: If length = 18, start = 77 and stop = 300 then\n",
    "            video is divided into 13 clips and frame number is returned.\n",
    "    \"\"\"\n",
    "    vid_sample = []\n",
    "    tac_sample = []\n",
    "    frames = np.arange(start,stop)\n",
    "    groups = len(frames)//length       #we will get these many clips from given video frames,\n",
    "    extra_frames = len(frames)%length  #need to fillup missing frames,\n",
    "    for index, i in enumerate(frames[::length]):\n",
    "        if(i+length<stop):\n",
    "            vid_sample.append(np.arange(i,i+length))\n",
    "            tac_sample.append(np.arange(index*length,index*length+length))\n",
    "        else:  #now append missing frames\n",
    "            end_frames = np.arange(i,stop)\n",
    "            missing_frames = length-end_frames.shape[0]\n",
    "            end_frames = np.append(end_frames, np.full((missing_frames,),stop))\n",
    "            vid_sample.append(end_frames)\n",
    "            tac_sample.append(np.arange(index*length,index*length+length))\n",
    "    if (vid_sample==[]):\n",
    "        raise ValueError(\"Not enough frames in a video\")\n",
    "    if (len(vid_sample) != groups and extra_frames and len(vid_sample) != (groups+1)):\n",
    "        raise ValueError(\"something went wrong. Expexted {} splits, but have only {}\".format(groups,len(sample)))\n",
    "    return vid_sample,tac_sample\n",
    "\n",
    "def get_offset(annotation_path, label_path):\n",
    "    \"\"\"\n",
    "    Get the offset in samples between video and tactile data\n",
    "    Args:   annotation_path - annotation.txt file containinf video frame alligned with label[0]\n",
    "            label_path - path of label.txt\n",
    "    \"\"\"\n",
    "    vid_fps = 18\n",
    "    tac_fps = 16.67\n",
    "    annotation = np.loadtxt(annotation_path)\n",
    "    label = np.loadtxt(label_path)\n",
    "#     print (annotation, label[0] * (vid_fps/tac_fps), label[2] * (vid_fps/tac_fps), int(annotation - label[0] * (vid_fps/tac_fps)),annotation_path)\n",
    "    return int(annotation - label[0] * (vid_fps/tac_fps))\n",
    "\n",
    "def get_label(pickup, drop, vid_frame, length, actual_label):\n",
    "    \"\"\"\n",
    "    Get the lable for given clip of video\n",
    "    Args:   pickup - label[0]\n",
    "            drop - label[2]\n",
    "            vid_frame - numpy array of video frames\n",
    "            length - length of clip\n",
    "            actual_lable - label[3]\n",
    "    \"\"\"\n",
    "    label = None\n",
    "    label_range = np.arange(pickup, drop)\n",
    "    #set lable to maximum frames follow\n",
    "    temp_label = np.zeros(length)\n",
    "    temp_label[np.in1d(vid_frame,label_range)]=1.0\n",
    "#     print(temp_label,vid_frame,label_range)\n",
    "    if(np.where(temp_label==1.0)[0].shape[0] > length//2):\n",
    "        label = actual_label\n",
    "    else:\n",
    "        label = 1\n",
    "#     print(label)\n",
    "    return np.array(label,dtype=float)\n",
    "\n",
    "def get_annotation(path):\n",
    "    return np.loadtxt(path)\n",
    "\n",
    "\n",
    "def make_train_dataset(data_path,req_frame_length):\n",
    "    data = []\n",
    "    for path in data_path:\n",
    "        front_vid_path, tac_path, pos_path, label_path, annotation_path = path[0], path[2], path[3], path[-1], path[6]\n",
    "        \n",
    "        # get the offset between video and tactile data\n",
    "        offset = get_offset(annotation_path, label_path)\n",
    "        # Read all the video frames\n",
    "        frames = glob.glob(os.path.join(front_vid_path,'*.jpg'))\n",
    "        #select video frame from offset to last frame\n",
    "        video_frames = np.arange(offset, len(frames)+1) \n",
    "        start,stop = video_frames[0], video_frames[-1]\n",
    "        if(len(video_frames) >= 430):\n",
    "            raise ValueError(\"more image frames = {} than tactile data for video {}\".format(video_frames, front_vid_path))\n",
    "        #collect subsequent frame numbers\n",
    "        vid_frames, tac_frames = split_frames(start, stop, req_frame_length)\n",
    "        print(vid_frames)\n",
    "        label = np.loadtxt(label_path)\n",
    "        annotation = get_annotation(annotation_path)\n",
    "        for vid_frame, tac_frame in zip(vid_frames, tac_frames):\n",
    "            pickup, drop = int(label[0]*18/16.67), int(label[2]*18/16.67)\n",
    "#             print(annotation, pickup,drop,annotation_path)\n",
    "            sequence_label = get_label(annotation, annotation+(drop-pickup), vid_frame, req_frame_length, label[3])\n",
    "            data.append((front_vid_path, tac_path, pos_path, label_path, vid_frame, tac_frame, sequence_label))\n",
    "    return data\n",
    "\n",
    "def make_test_dataset(data_path,req_frame_length):\n",
    "    data = []\n",
    "    for path in data_path:\n",
    "        front_vid_path, tac_path, pos_path, label_path, annotation_path = path[0], path[2], path[3], path[-1], path[6]\n",
    "        \n",
    "        # get the offset between video and tactile data\n",
    "        offset = get_offset(annotation_path, label_path)\n",
    "        # Read all the video frames\n",
    "        frames = glob.glob(os.path.join(front_vid_path,'*.jpg'))\n",
    "        #select video frame from offset to last frame\n",
    "        video_frames = np.arange(offset, len(frames)+1) \n",
    "        start,stop = video_frames[0], video_frames[-1]\n",
    "        if(len(video_frames) >= 430):\n",
    "            raise ValueError(\"more image frames = {} than tactile data for video {}\".format(video_frames, front_vid_path))\n",
    "        #collect subsequent frame numbers\n",
    "        vid_frames, tac_frames = split_frames(start, stop, req_frame_length)\n",
    "        label = np.loadtxt(label_path)\n",
    "        annotation = get_annotation(annotation_path)\n",
    "        pickup, drop = int(label[0]*18/16.67), int(label[2]*18/16.67)\n",
    "        sequence_label = []\n",
    "        for vid_frame in vid_frames:\n",
    "#             print(annotation, pickup,drop,annotation_path)\n",
    "            sequence_label.append(get_label(annotation, annotation+(drop-pickup), vid_frame, req_frame_length, label[3]))\n",
    "        data.append((front_vid_path, tac_path, pos_path, label_path, vid_frames, tac_frames, sequence_label))\n",
    "    return data\n",
    "\n",
    "def get_location(root, split_file):\n",
    "    data_path = []\n",
    "    file_path = np.loadtxt(split_file,dtype=str)\n",
    "    prefix = root.split(file_path[0][:32])[0]\n",
    "    file_names = ['images/front_rgb/','images/left_rgb/','tactile.txt','pos.txt','label.txt','flow', 'video_grasp_timestamp.txt']\n",
    "    for i,file in enumerate(file_path):\n",
    "        front_video_path = prefix + file + file_names[0]\n",
    "        left_video_path = prefix + file + file_names[1]\n",
    "        tactile_path = prefix +\"/Visual-Tactile_Dataset/tactile_data/\"+file.split(file[:32])[1] + file_names[2]\n",
    "        pos_path = prefix + file + file_names[3]\n",
    "        label_path = prefix + file + file_names[4]\n",
    "        front_flow_path = prefix + file + file_names[5] + '/' + 'front_rgb/'\n",
    "        left_flow_path = prefix + file + file_names[5] + '/' + 'left_rgb/'\n",
    "        annotation_path = prefix + \"/Visual-Tactile_Dataset/dataset_annotations/\"+file.split(file[:32])[1] + file_names[6]\n",
    "        data_path.append((front_video_path, left_video_path, tactile_path, pos_path, \\\n",
    "                       front_flow_path, left_flow_path, annotation_path, label_path))\n",
    "    return data_path\n",
    "\n",
    "\n",
    "\n",
    "class VisualTactile(data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset to load sequence of framses frm the video\n",
    "    root = directory of ur datasets\n",
    "    split_file = your test or train split .txt file\n",
    "    transforms = Transform if you wants to apply to video\n",
    "    frames_to_load = 18 default, sequence of frames to load\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, split_file, transforms=None, frames_to_load = 18):\n",
    "        self.root = root\n",
    "        self.split_file = split_file\n",
    "        self.transforms = transforms\n",
    "        self.frames_to_load = frames_to_load\n",
    "        # get path of each file data you would like to process\n",
    "        self.data_path = get_location(self.root, self.split_file)\n",
    "        if len(self.data_path) == 0:\n",
    "            raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root))\n",
    "\n",
    "        # get path and frames for training data\n",
    "        if(\"train\" in split_file):\n",
    "            self.clip_data = make_train_dataset(self.data_path, self.frames_to_load)\n",
    "        else:\n",
    "            self.video_data = make_test_dataset(self.data_path, self.frames_to_load)\n",
    "            \n",
    "#     def __getitem__(self,clip_index):\n",
    "#         front_vid_path, tac_path, pos_path, label_path, vid_frames, tac_frames, label = self.clip_data[clip_index]\n",
    "#         clip = video_loader(front_vid_path, vid_frames)\n",
    "# #         tactile = tactile_loader(tac_path, tac_frames)\n",
    "# #         postion = pos_loader(pos_path, tac_frames)\n",
    "#         return video_to_tensor(clip), torch.from_numpy(label)\n",
    "\n",
    "    def __getitem__(self,clip_index):\n",
    "        front_vid_path, tac_path, pos_path, label_path, vid_frames, tac_frames, label = self.clip_data[clip_index]\n",
    "        clip, label = self.custom_getitem(front_vid_path, vid_frames, label)\n",
    "        return clip, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.clip_data)\n",
    "\n",
    "    def custom_getitem(self,front_vid_path, vid_frames, label, tac_path=None, pos_path=None, tac_frames=None):\n",
    "        clip = video_loader(front_vid_path, vid_frames)\n",
    "#         tactile = tactile_loader(tac_path, tac_frames)\n",
    "#         postion = pos_loader(pos_path, tac_frames)\n",
    "        if (self.transforms is not None):\n",
    "            clip = self.transforms(clip)\n",
    "        return video_to_tensor(clip), torch.from_numpy(label)\n",
    "\n",
    "    def get_video_frames(self, video_index):\n",
    "        front_vid_path, tac_path, pos_path, label_path, vid_frames, tac_frames, label   = self.video_data[video_index]\n",
    "        data = []\n",
    "        for vid, tac, lab in zip(vid_frames, tac_frames, label):\n",
    "            data.append(self.custom_getitem(front_vid_path, vid, lab))\n",
    "        return data, label_path\n",
    "\n",
    "    def get_num_videos(self):\n",
    "        return len(self.video_data)\n",
    "# obj = VisualTactile(\"../../../t/Visual-Tactile_Dataset/dataset/\", \"../master_i3d/trainv2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_offset(annotation_path, label_path):\n",
    "    vid_fps = 18\n",
    "    tac_fps = 16.67\n",
    "    annotation = np.loadtxt(annotation_path)\n",
    "    label = np.loadtxt(label_path)\n",
    "    return int(annotation - label[0] * (vid_fps/tac_fps))\n",
    "\n",
    "path = '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/right/0/'\n",
    "vid_path = '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/right/0/images/front_rgb/'\n",
    "annotation_path = '../../../Visual-Tactile_Dataset/dataset_annotations/Cheez/50_432/right/0/video_grasp_timestamp.txt'\n",
    "\n",
    "\n",
    "req_frame_length = 18\n",
    "\n",
    "label = np.loadtxt(path+\"label.txt\")\n",
    "offset = get_offset(annotation_path, path+\"label.txt\")\n",
    "drop = label[0]*18/16.67\n",
    "\n",
    "frames = glob.glob(os.path.join(vid_path,'*.jpg'))\n",
    "\n",
    "sample = []\n",
    "image_frames = np.arange(offset, len(frames)+1)\n",
    "start,stop = image_frames[0], image_frames[-1]\n",
    "if(len(image_frames) >= 400):\n",
    "    raise ValueError(\"more image frames = {} than tactile data\".format(image_frames))\n",
    "\n",
    "split = len(image_frames)//req_frame_length\n",
    "extra_frames = len(image_frames)%req_frame_length\n",
    "sample, tac = split_frames(start, stop, req_frame_length)\n",
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_frames(start,stop,length):\n",
    "    vid_sample = []\n",
    "    tac_sample = []\n",
    "    frames = np.arange(start,stop)\n",
    "    groups = len(frames)//length       #we will get these many clips from given video frames,\n",
    "    extra_frames = len(frames)%length  #need to fillup missing frames,\n",
    "    for index, i in enumerate(frames[::length]):\n",
    "        if(i+length<stop):\n",
    "            vid_sample.append(np.arange(i,i+length))\n",
    "            tac_sample.append(np.arange(index*length,index*length+length))\n",
    "        else:  #now append missing frames\n",
    "            end_frames = np.arange(i,stop)\n",
    "            missing_frames = length-end_frames.shape[0]\n",
    "            end_frames = np.append(end_frames, np.full((missing_frames,),stop))\n",
    "            vid_sample.append(end_frames)\n",
    "            tac_sample.append(np.arange(index*length,index*length+length))\n",
    "    if (vid_sample==[]):\n",
    "        raise ValueError(\"Not enough frames in a video\")\n",
    "    if (len(vid_sample) != groups and extra_frames and len(vid_sample) != (groups+1)):\n",
    "        raise ValueError(\"something went wrong. Expexted {} splits, but have only {}\".format(groups,len(sample)))\n",
    "    return vid_sample,tac_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing dataset code (for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,\n",
      "        96,  97,  98,  99, 100]), array([101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113,\n",
      "       114, 115, 116, 117, 118]), array([119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131,\n",
      "       132, 133, 134, 135, 136]), array([137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149,\n",
      "       150, 151, 152, 153, 154]), array([155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "       168, 169, 170, 171, 172]), array([173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185,\n",
      "       186, 187, 188, 189, 190]), array([191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203,\n",
      "       204, 205, 206, 207, 208]), array([209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
      "       222, 223, 224, 225, 226]), array([227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "       240, 241, 242, 243, 244]), array([245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257,\n",
      "       258, 259, 260, 261, 262]), array([263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275,\n",
      "       276, 277, 278, 279, 280]), array([281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "       294, 295, 296, 297, 298]), array([299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316]), array([317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329,\n",
      "       330, 331, 332, 333, 334]), array([335, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336,\n",
      "       336, 336, 336, 336, 336])]\n",
      "[array([82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,\n",
      "       99]), array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
      "       113, 114, 115, 116, 117]), array([118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
      "       131, 132, 133, 134, 135]), array([136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148,\n",
      "       149, 150, 151, 152, 153]), array([154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166,\n",
      "       167, 168, 169, 170, 171]), array([172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184,\n",
      "       185, 186, 187, 188, 189]), array([190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202,\n",
      "       203, 204, 205, 206, 207]), array([208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225]), array([226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
      "       239, 240, 241, 242, 243]), array([244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256,\n",
      "       257, 258, 259, 260, 261]), array([262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274,\n",
      "       275, 276, 277, 278, 279]), array([280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292,\n",
      "       293, 294, 295, 296, 297]), array([298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310,\n",
      "       311, 312, 313, 314, 315]), array([316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328,\n",
      "       329, 330, 331, 332, 333])]\n"
     ]
    }
   ],
   "source": [
    "path1 = ('../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/images/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/images/left_rgb/', '../../../t/Visual-Tactile_Dataset/tactile_data/Cheez/50_432/top/0/tactile.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/pos.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/flow/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/flow/left_rgb/', '../../../t/Visual-Tactile_Dataset/dataset_annotations/Cheez/50_432/top/0/video_grasp_timestamp.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/label.txt')\n",
    "path2 = ('../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/images/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/images/left_rgb/', '../../../t/Visual-Tactile_Dataset/tactile_data/Cheez/50_432/top/7/tactile.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/pos.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/flow/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/flow/left_rgb/', '../../../t/Visual-Tactile_Dataset/dataset_annotations/Cheez/50_432/top/7/video_grasp_timestamp.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/label.txt')\n",
    "path = [path1,path2]\n",
    "data = make_train_dataset(path, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing dataset code (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path1 = ('../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/images/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/images/left_rgb/', '../../../t/Visual-Tactile_Dataset/tactile_data/Cheez/50_432/top/0/tactile.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/pos.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/flow/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/flow/left_rgb/', '../../../t/Visual-Tactile_Dataset/dataset_annotations/Cheez/50_432/top/0/video_grasp_timestamp.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/label.txt')\n",
    "path2 = ('../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/images/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/images/left_rgb/', '../../../t/Visual-Tactile_Dataset/tactile_data/Cheez/50_432/top/7/tactile.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/pos.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/flow/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/flow/left_rgb/', '../../../t/Visual-Tactile_Dataset/dataset_annotations/Cheez/50_432/top/7/video_grasp_timestamp.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/label.txt')\n",
    "path = [path1,path2]\n",
    "data = make_test_dataset(path, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Videoloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "front_vid_path, tac_path, pos_path, label_path, vid_frames, tac_frames, label = data[0]\n",
    "inp = video_loader(front_vid_path, vid_frames)\n",
    "inp = video_to_tensor(inp)\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[0][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Resnet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import resnet\n",
    "import torch\n",
    "from torch import nn\n",
    "import dataset\n",
    "\n",
    "pretrained_path = \"../../../out/resnet/resnet-18-kinetics.pth\"\n",
    "model = resnet.resnet18(sample_size = 112, sample_duration = 18, num_classes = 400, shortcut_type='A')\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "pretrain = torch.load(pretrained_path, map_location=\"cpu\")\n",
    "model.load_state_dict(pretrain['state_dict'])\n",
    "\n",
    "x = torch.autograd.Variable(inp)\n",
    "x = x.unsqueeze(0)\n",
    "out = model(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc = nn.Linear(1024, 1)\n",
    "fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.arange(18)\n",
    "b = np.arange(18,36)\n",
    "c = np.zeros(18)\n",
    "for i,j,k in zip(a,b,c):\n",
    "    print(i,j,k)\n",
    "    \n",
    "a = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def custom_getitem(front_vid_path, vid_frames, label, tac_path=None, pos_path=None, tac_frames=None):\n",
    "    clip = video_loader(front_vid_path, vid_frames)\n",
    "#         tactile = tactile_loader(tac_path, tac_frames)\n",
    "#         postion = pos_loader(pos_path, tac_frames)\n",
    "    return video_to_tensor(clip), torch.from_numpy(label)\n",
    "\n",
    "def get_video_frames(video_index,video_data):\n",
    "    front_vid_path, tac_path, pos_path, label_path, vid_frames, tac_frames, label   = video_data[video_index]\n",
    "    data = []\n",
    "    for vid, tac, label in zip(vid_frames, tac_frames, label):\n",
    "        print(vid)\n",
    "        data.append(custom_getitem(front_vid_path, vid_frames, label))\n",
    "    return data, label_path\n",
    "\n",
    "get_video_frames(0,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TACTILE_MAX_MAGNITUDE = np.array([23655, 20662, 14496,  6475, 41133, 64793, 59317, 33177, 19897,\n",
    "       62084, 49874, 29170, 42944, 14976, 12311, 14331])\n",
    "\n",
    "def tactile_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    output =[]\n",
    "    groups = []\n",
    "    tactile_frame = np.loadtxt(path)\n",
    "    tactile = tactile_frame.astype('float')\n",
    "    tactile = tactile/TACTILE_MAX_MAGNITUDE\n",
    "    #group of 4 sensors\n",
    "    n=4\n",
    "    out = [tactile[:,k:k+n] for k in range(0, tactile.shape[1], n)]\n",
    "    for data in out:\n",
    "        for i in range(data.shape[1]):\n",
    "            groups.append(tactile[:324,i].reshape(18,18,1))\n",
    "        temp = video_to_tensor(np.array(groups))\n",
    "        output.append(temp)\n",
    "        groups = []\n",
    "        \n",
    "        \n",
    "    output =[] \n",
    "    TACTILE_TIME = 24\n",
    "    UPSAMPLE_FREQ = 18\n",
    "    up_samples=TACTILE_TIME*UPSAMPLE_FREQ\n",
    "    with open(path, 'rb') as f:\n",
    "#         tactile_frame = pd.read_csv(f,delimiter=' ', header=None)\n",
    "#         tactile = tactile_frame.as_matrix()\n",
    "        tactile_frame = np.loadtxt(f)\n",
    "        tactile = tactile_frame.astype('float')\n",
    "        tactile = signal.resample(tactile,up_samples)\n",
    "        tactile[np.where(tactile<0.1)] = 0.  # remove all the negative samples with 0\n",
    "        tactile = tactile/TACTILE_MAX_MAGNITUDE #normalize input in range of 0 to 1.\n",
    "        for frame in frames:\n",
    "            output.append(tactile[frame,:].reshape(4,4,1))\n",
    "        return np.array(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
