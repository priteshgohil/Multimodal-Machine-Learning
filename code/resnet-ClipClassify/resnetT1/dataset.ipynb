{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "if('/opt/ros/kinetic/lib/python2.7/dist-packages' in sys.path):\n",
    "    sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "import cv2\n",
    "\n",
    "\n",
    "TACTILE_MAX_MAGNITUDE = np.array([23655, 20662, 14496,  6475, 41133, 64793, 59317, 33177, 19897,\n",
    "       62084, 49874, 29170, 42944, 14976, 12311, 14331])\n",
    "POS_MAX_MAGNITUDE = np.array([ 20,  91,  25, 109,  54,  87,  12,  21])\n",
    "TAC_IMAGE_INDEX = np.array([[12,8,9,10,11],\n",
    "                          [13,0,1,2,3],\n",
    "                          [15,4,5,6,7],\n",
    "                          [14,8,9,10,11]])\n",
    "POS_IMAGE_INDEX = np.array([[0,6,7,0],\n",
    "                  [5,1,3,5],\n",
    "                  [4,0,2,4],\n",
    "                  [0,6,7,0]])\n",
    "TACTILE_TIME = 24\n",
    "UPSAMPLE_FREQ = 18\n",
    "\n",
    "def video_to_tensor(frames):\n",
    "    \"\"\"Convert a ``numpy.ndarray`` to tensor.\n",
    "    Converts a numpy.ndarray (T x H x W x C)\n",
    "    to a torch.FloatTensor of shape (C x T x H x W)\n",
    "\n",
    "    Args:\n",
    "         pic (numpy.ndarray): Video to be converted to tensor.\n",
    "    Returns:\n",
    "         Tensor: Converted video.\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(frames.transpose([3,0,1,2]))\n",
    "\n",
    "def preprocess_image(cv_frame, size):\n",
    "    \"\"\"\n",
    "    Args:   frame - cv image\n",
    "            size - tuple of (W x H)\n",
    "    \"\"\"\n",
    "\n",
    "    im = cv2.resize(cv_frame, size) #compress image\n",
    "    im = np.array(Image.fromarray(cv2.cvtColor(im,cv2.COLOR_BGR2RGB))) #convert to RGB image\n",
    "    im = im/255 #normalize\n",
    "    im = (im-0.5)/0.5\n",
    "    return im\n",
    "\n",
    "def video_loader(vid_path, frames):\n",
    "    \"\"\"\n",
    "    Args:   vid_path - location of saved video frames\n",
    "            frames - numpy array of frame number to read\n",
    "    \"\"\"\n",
    "    resize_W = 160\n",
    "    resize_H = 120\n",
    "    images = []\n",
    "    for frame in frames:\n",
    "        image_name = \"frame_{:04}.jpg\".format(frame)\n",
    "        # image_name = \"frame_\" + str(frame).zfill(4) + \".jpg\"\n",
    "#         image_name = \"frame_\" + f'{frame:04}' + \".jpg\"\n",
    "        # fixing bug of padding zero_pads.\n",
    "        if image_name[6]=='-':\n",
    "            print(\"wrong zerp padding detected {} , {}\".format(image_name, vid_path))\n",
    "            image_name = image_name.replace('-','0')\n",
    "\n",
    "        img = cv2.imread(vid_path + image_name)\n",
    "        ## DEBUG:\n",
    "        #print(vid_path + image_name)\n",
    "        #print(img.shape)\n",
    "        if (img is not None):\n",
    "            img = preprocess_image(img, (resize_W,resize_H))\n",
    "            images.append(img)\n",
    "        else:\n",
    "            print(vid_path, image_name, img)\n",
    "            raise ValueError(\"something went wrong. Expexted {}, {}, {}\".format(vid_path, image_name, img))\n",
    "    return np.array(images)\n",
    "\n",
    "def tactile_loader(tac_path, frames):\n",
    "    output =[]\n",
    "    tac_data = read_tactile_data(tac_path)\n",
    "    for frame in frames:\n",
    "        t = tac_data[int(frame),:]\n",
    "        output.append(t[TAC_IMAGE_INDEX.ravel()].reshape(4,5,1))\n",
    "    return np.array(output)\n",
    "\n",
    "def read_tactile_data(tac_path):\n",
    "    up_samples=TACTILE_TIME*UPSAMPLE_FREQ\n",
    "    with open(tac_path, 'rb') as f:\n",
    "        tactile_frame = np.loadtxt(f)\n",
    "        tactile = tactile_frame.astype('float')\n",
    "        tactile = signal.resample(tactile,up_samples)\n",
    "        tactile[np.where(tactile<0.1)] = 0.  # remove all the negative samples with 0\n",
    "        tactile = tactile/TACTILE_MAX_MAGNITUDE #normalize input in range of 0 to 1.\n",
    "        tactile = 2*tactile - 1 # normalize in the range of 0 to 1\n",
    "    return tactile\n",
    "\n",
    "def pos_loader(pos_path, frames):\n",
    "    output =[]\n",
    "    pos = read_pos_data(pos_path)\n",
    "    for frame in frames:\n",
    "        p = pos[int(frame),:]\n",
    "        out = p[POS_IMAGE_INDEX.ravel()].reshape(4,4,1)\n",
    "        out[0,0] = 0.\n",
    "        out[3,0] = 0.\n",
    "        out[0,3] = 0.\n",
    "        out[3,3] = 0.\n",
    "        output.append(out)\n",
    "        # output.append(p[POS_IMAGE_INDEX.ravel()].reshape(4,4,1))\n",
    "    return np.array(output)\n",
    "\n",
    "def read_pos_data(pos_path):\n",
    "    up_samples=TACTILE_TIME*UPSAMPLE_FREQ\n",
    "    with open(pos_path, 'rb') as f:\n",
    "        pos_frame = np.loadtxt(f)\n",
    "        pos = pos_frame.astype('float')\n",
    "        pos = signal.resample(pos,up_samples)\n",
    "        pos = pos/POS_MAX_MAGNITUDE #normalize input in range of 0 to 1.\n",
    "    return pos\n",
    "\n",
    "def split_frames(start,stop,length):\n",
    "    \"\"\"\n",
    "    To match video frames with tactile,\n",
    "    Input:  start - starting frame of video\n",
    "            stop - last frame of video\n",
    "    output: sequence of frames in batch of length.\n",
    "    for ex: If length = 18, start = 77 and stop = 300 then\n",
    "            video is divided into 13 clips and frame number is returned.\n",
    "    \"\"\"\n",
    "    vid_sample = []\n",
    "    tac_sample = []\n",
    "    frames = np.arange(start,stop)\n",
    "    groups = len(frames)//length       #we will get these many clips from given video frames,\n",
    "    extra_frames = len(frames)%length  #need to fillup missing frames,\n",
    "    for index, i in enumerate(frames[::length]):\n",
    "        if(i+length<stop):\n",
    "            vid_sample.append(np.arange(i,i+length))\n",
    "            tac_sample.append(np.arange(index*length,index*length+length))\n",
    "        else:  #now append missing frames\n",
    "            end_frames = np.arange(i,stop)\n",
    "            missing_frames = length-end_frames.shape[0]\n",
    "            end_frames = np.append(end_frames, np.full((missing_frames,),stop))\n",
    "            vid_sample.append(end_frames)\n",
    "            tac_sample.append(np.arange(index*length,index*length+length))\n",
    "    if (vid_sample==[]):\n",
    "        raise ValueError(\"Not enough frames in a video\")\n",
    "    if (len(vid_sample) != groups and extra_frames and len(vid_sample) != (groups+1)):\n",
    "        raise ValueError(\"something went wrong. Expexted {} splits, but have only {}\".format(groups,len(sample)))\n",
    "    return vid_sample,tac_sample\n",
    "\n",
    "def get_offset(an_path, lbl_path):\n",
    "    \"\"\"\n",
    "    Get the offset in samples between video and tactile data\n",
    "    Args:   annotation_path - annotation.txt file containinf video frame alligned with label[0]\n",
    "            label_path - path of label.txt\n",
    "    \"\"\"\n",
    "    vid_fps = 18\n",
    "    tac_fps = 16.67\n",
    "    a = np.loadtxt(an_path)\n",
    "    l = np.loadtxt(lbl_path)\n",
    "#     print (annotation, label[0] * (vid_fps/tac_fps), label[2] * (vid_fps/tac_fps), int(annotation - label[0] * (vid_fps/tac_fps)),annotation_path)\n",
    "    offset = int(a - l[0] * (vid_fps/tac_fps))\n",
    "    if(offset <= 0):\n",
    "        # raise ValueError(\"something wrong{},{}:{},{}\".format(a,l[0] * (vid_fps/tac_fps),lbl_path, an_path))\n",
    "        offset = 1\n",
    "    return offset\n",
    "\n",
    "def get_label(pickup, drop, vid_frame, length, actual_label):\n",
    "    \"\"\"\n",
    "    Get the lable for given clip of video\n",
    "    Args:   pickup - label[0]\n",
    "            drop - label[2]\n",
    "            vid_frame - numpy array of video frames\n",
    "            length - length of clip\n",
    "            actual_lable - label[3]\n",
    "    \"\"\"\n",
    "    label = None\n",
    "    label_range = np.arange(pickup, drop)\n",
    "    #set lable to maximum frames follow\n",
    "    temp_label = np.zeros(length)\n",
    "    temp_label[np.in1d(vid_frame,label_range)]=1.0\n",
    "#     print(temp_label,vid_frame,label_range)\n",
    "    if(np.where(temp_label==1.0)[0].shape[0] > length//2):\n",
    "        label = actual_label\n",
    "    else:\n",
    "        label = 1\n",
    "#     print(label)\n",
    "    return np.array(label,dtype=float)\n",
    "\n",
    "def get_annotation(path):\n",
    "    return np.loadtxt(path)\n",
    "\n",
    "\n",
    "def make_train_dataset(data_path,req_frame_length):\n",
    "    data = []\n",
    "    for path in data_path:\n",
    "        front_vid_path, tac_path, pos_path, label_path, annotation_path = path[0], path[2], path[3], path[-1], path[6]\n",
    "\n",
    "        # get the offset between video and tactile data\n",
    "        offset = get_offset(annotation_path, label_path)\n",
    "        # Read all the video frames\n",
    "        frames = glob.glob(os.path.join(front_vid_path,'*.jpg'))\n",
    "        #select video frame from offset to last frame\n",
    "        video_frames = np.arange(offset, len(frames)+1)\n",
    "        start,stop = video_frames[0], video_frames[-1]\n",
    "        if(len(video_frames) >= 430):\n",
    "            raise ValueError(\"more image frames = {} than tactile data for video {}\".format(video_frames, front_vid_path))\n",
    "        #collect subsequent frame numbers\n",
    "        vid_frames, tac_frames = split_frames(start, stop, req_frame_length)\n",
    "        label = np.loadtxt(label_path)\n",
    "        annotation = get_annotation(annotation_path)\n",
    "        for vid_frame, tac_frame in zip(vid_frames, tac_frames):\n",
    "            pickup, drop = int(label[0]*18/16.67), int(label[2]*18/16.67)\n",
    "#             print(annotation, pickup,drop,annotation_path)\n",
    "            sequence_label = get_label(annotation, annotation+(drop-pickup), vid_frame, req_frame_length, label[3])\n",
    "            data.append((front_vid_path, tac_path, pos_path, label_path, vid_frame, tac_frame, sequence_label))\n",
    "    return data\n",
    "\n",
    "def make_test_dataset(data_path,req_frame_length):\n",
    "    data = []\n",
    "    for path in data_path:\n",
    "        front_vid_path, tac_path, pos_path, label_path, annotation_path = path[0], path[2], path[3], path[-1], path[6]\n",
    "\n",
    "        # get the offset between video and tactile data\n",
    "        offset = get_offset(annotation_path, label_path)\n",
    "        # Read all the video frames\n",
    "        frames = glob.glob(os.path.join(front_vid_path,'*.jpg'))\n",
    "        #select video frame from offset to last frame\n",
    "        video_frames = np.arange(offset, len(frames)+1)\n",
    "        start,stop = video_frames[0], video_frames[-1]\n",
    "        if(len(video_frames) >= 430):\n",
    "            raise ValueError(\"more image frames = {} than tactile data for video {}\".format(video_frames, front_vid_path))\n",
    "        #collect subsequent frame numbers\n",
    "        vid_frames, tac_frames = split_frames(start, stop, req_frame_length)\n",
    "        label = np.loadtxt(label_path)\n",
    "        annotation = get_annotation(annotation_path)\n",
    "        pickup, drop = int(label[0]*18/16.67), int(label[2]*18/16.67)\n",
    "        sequence_label = []\n",
    "        for vid_frame in vid_frames:\n",
    "#             print(annotation, pickup,drop,annotation_path)\n",
    "            sequence_label.append(get_label(annotation, annotation+(drop-pickup), vid_frame, req_frame_length, label[3]))\n",
    "        data.append((front_vid_path, tac_path, pos_path, label_path, vid_frames, tac_frames, sequence_label))\n",
    "    return data\n",
    "\n",
    "def get_location(root, split_file):\n",
    "    data_path = []\n",
    "    file_path = np.loadtxt(split_file,dtype=str)\n",
    "    prefix = root.split(file_path[0][:32])[0]\n",
    "    file_names = ['images/front_rgb/','images/left_rgb/','tactile.txt','pos.txt','label.txt','flow', 'video_grasp_timestamp.txt']\n",
    "    for i,file in enumerate(file_path):\n",
    "        front_video_path = prefix + file + file_names[0]\n",
    "        left_video_path = prefix + file + file_names[1]\n",
    "        tactile_path = prefix +\"/Visual-Tactile_Dataset/tactile_data/\"+file.split(file[:32])[1] + file_names[2]\n",
    "        pos_path = prefix + file + file_names[3]\n",
    "        label_path = prefix + file + file_names[4]\n",
    "        front_flow_path = prefix + file + file_names[5] + '/' + 'front_rgb/'\n",
    "        left_flow_path = prefix + file + file_names[5] + '/' + 'left_rgb/'\n",
    "        annotation_path = prefix + \"/Visual-Tactile_Dataset/dataset_annotations/\"+file.split(file[:32])[1] + file_names[6]\n",
    "        data_path.append((front_video_path, left_video_path, tactile_path, pos_path, \\\n",
    "                       front_flow_path, left_flow_path, annotation_path, label_path))\n",
    "    return data_path\n",
    "\n",
    "\n",
    "\n",
    "class VisualTactile(data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset to load sequence of framses frm the video\n",
    "    root = directory of ur datasets\n",
    "    split_file = your test or train split .txt file\n",
    "    transforms = Transform if you wants to apply to video\n",
    "    frames_to_load = 18 default, sequence of frames to load\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, split_file, transforms=None, frames_to_load = 18):\n",
    "        self.root = root\n",
    "        self.split_file = split_file\n",
    "        self.transforms = transforms\n",
    "        self.frames_to_load = frames_to_load\n",
    "        # get path of each file data you would like to process\n",
    "        self.data_path = get_location(self.root, self.split_file)\n",
    "        self.num_test_clips = []\n",
    "        if len(self.data_path) == 0:\n",
    "            raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root))\n",
    "\n",
    "        # get path and frames for training data\n",
    "        if(\"train\" in split_file):\n",
    "            self.clip_data = make_train_dataset(self.data_path, self.frames_to_load)\n",
    "        else:\n",
    "            self.video_data = make_test_dataset(self.data_path, self.frames_to_load)\n",
    "\n",
    "\n",
    "    def __getitem__(self,clip_index):\n",
    "        front_vid_path, tac_path, pos_path, label_path, vid_frames, tac_frames, label = self.clip_data[clip_index]\n",
    "        # data = self.custom_getitem(front_vid_path, vid_frames, label, tac_path, pos_path, tac_frames)\n",
    "        # return data, label_path\n",
    "        clip, tactile, position, label = self.custom_getitem(front_vid_path, vid_frames, label, tac_path, pos_path, tac_frames)\n",
    "        return clip, tactile, position, label, label_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clip_data)\n",
    "\n",
    "    def custom_getitem(self,front_vid_path, vid_frames, label, tac_path=None, pos_path=None, tac_frames=None):\n",
    "        clip = video_loader(front_vid_path, vid_frames)\n",
    "        tactile = tactile_loader(tac_path, tac_frames)\n",
    "        position = pos_loader(pos_path, tac_frames)\n",
    "        if (self.transforms is not None):\n",
    "           clip = self.transforms(clip)\n",
    "        return video_to_tensor(clip),video_to_tensor(tactile),video_to_tensor(position), torch.from_numpy(label)\n",
    "\n",
    "    def get_video_frames(self, video_index):\n",
    "        front_vid_path, tac_path, pos_path, label_path, vid_frames, tac_frames, label = self.video_data[video_index]\n",
    "        data = []\n",
    "        for vid, tac, lab in zip(vid_frames, tac_frames, label):\n",
    "            data.append(self.custom_getitem(front_vid_path, vid, lab, tac_path, pos_path, tac))\n",
    "        return data, label_path\n",
    "\n",
    "    def get_num_videos(self):\n",
    "        return len(self.video_data)\n",
    "\n",
    "    def get_num_clips(self):\n",
    "        for i in self.video_data:\n",
    "            self.num_test_clips.append(len(i[4]))\n",
    "        return sum(self.num_test_clips)\n",
    "# obj = VisualTactile(\"../../../t/Visual-Tactile_Dataset/dataset/\", \"../master_i3d/trainv2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing dataset code (for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path1 = ('../../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/images/front_rgb/', '../../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/images/left_rgb/', '../../../../t/Visual-Tactile_Dataset/tactile_data/Cheez/50_432/top/0/tactile.txt', '../../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/pos.txt', '../../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/flow/front_rgb/', '../../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/flow/left_rgb/', '../../../../t/Visual-Tactile_Dataset/dataset_annotations/Cheez/50_432/top/0/video_grasp_timestamp.txt', '../../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/label.txt')\n",
    "path2 = ('../../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/images/front_rgb/', '../../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/images/left_rgb/', '../../../../t/Visual-Tactile_Dataset/tactile_data/Cheez/50_432/top/7/tactile.txt', '../../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/pos.txt', '../../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/flow/front_rgb/', '../../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/flow/left_rgb/', '../../../../t/Visual-Tactile_Dataset/dataset_annotations/Cheez/50_432/top/7/video_grasp_timestamp.txt', '../../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/label.txt')\n",
    "# path1 = ('../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/images/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/images/left_rgb/', '../../../t/Visual-Tactile_Dataset/tactile_data/Cheez/50_432/top/0/tactile.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/pos.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/flow/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/flow/left_rgb/', '../../../t/Visual-Tactile_Dataset/dataset_annotations/Cheez/50_432/top/0/video_grasp_timestamp.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/label.txt')\n",
    "# path2 = ('../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/images/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/images/left_rgb/', '../../../t/Visual-Tactile_Dataset/tactile_data/Cheez/50_432/top/7/tactile.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/pos.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/flow/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/flow/left_rgb/', '../../../t/Visual-Tactile_Dataset/dataset_annotations/Cheez/50_432/top/7/video_grasp_timestamp.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/label.txt')\n",
    "path = [path1,path2]\n",
    "data = make_train_dataset(path, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing dataset code (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path1 = ('../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/images/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/images/left_rgb/', '../../../t/Visual-Tactile_Dataset/tactile_data/Cheez/50_432/top/0/tactile.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/pos.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/flow/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/flow/left_rgb/', '../../../t/Visual-Tactile_Dataset/dataset_annotations/Cheez/50_432/top/0/video_grasp_timestamp.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/0/label.txt')\n",
    "path2 = ('../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/images/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/images/left_rgb/', '../../../t/Visual-Tactile_Dataset/tactile_data/Cheez/50_432/top/7/tactile.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/pos.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/flow/front_rgb/', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/flow/left_rgb/', '../../../t/Visual-Tactile_Dataset/dataset_annotations/Cheez/50_432/top/7/video_grasp_timestamp.txt', '../../../t/Visual-Tactile_Dataset/dataset/Cheez/50_432/top/7/label.txt')\n",
    "path = [path1,path2]\n",
    "data = make_test_dataset(path, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Videoloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "front_vid_path, tac_path, pos_path, label_path, vid_frames, tac_frames, label = data[0]\n",
    "inp = video_loader(front_vid_path, vid_frames)\n",
    "inp = video_to_tensor(inp)\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[0][6]\n",
    "# video_loader(data[0][0],data[0][4]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing tactile Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tactile_path = data[10][1]\n",
    "sensor_frames = data[10][-2]\n",
    "tac_img = tactile_loader(tactile_path,sensor_frames)\n",
    "# tac_img[0].shape\n",
    "fig = plt.figure(figsize=(5,4),frameon=False)\n",
    "\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "\n",
    "# ax.imshow(your_image, aspect='normal')\n",
    "# fig.savefig(fname, dpi)\n",
    "# ax.imshow(tac_img[0].reshape(4,5),cmap=\"gray\")\n",
    "# # plt.axis(\"off\")\n",
    "# plt.savefig(\"../../../../t/Visual-Tactile_Dataset/tactile_image/t{}.png\".format(i))\n",
    "\n",
    "for i, img in enumerate(tac_img):\n",
    "    ax.imshow(img.reshape(4,5), cmap=\"gray\")\n",
    "#     plt.savefig(\"../../../../t/Visual-Tactile_Dataset/tactile_image/t{}.png\".format(i))\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visaulizing pos image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_path = data[5][2]\n",
    "sensor_frames = data[5][-2]\n",
    "pos_img = pos_loader(pos_path,sensor_frames)\n",
    "\n",
    "fig = plt.figure(figsize=(4,4),frameon=False)\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "\n",
    "for i, img in enumerate(pos_img):\n",
    "    ax.imshow(img.reshape(4,4), cmap=\"gray\")\n",
    "    plt.savefig(\"../../../../t/Visual-Tactile_Dataset/pos_image/p{}.png\".format(i))\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing fused Tactile + Pos image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TAC_IMAGE_INDEX = np.array([[0,15,0,0,11,0,0,0,0],\n",
    "                  [0,12,8,0,9,0,10,11,0],\n",
    "                  [0,13,0,0,1,2,0,3,0],\n",
    "                  [0,14,4,0,5,0,6,7,4],\n",
    "                  [9,15,6,0,15,0,9,0,0]])\n",
    "POS_IMAGE_INDEX = np.array([[0,0,5,4,0,5,2,3,4],\n",
    "                  [4,0,0,2,0,1,0,0,0],\n",
    "                  [6,0,0,0,0,0,5,0,1],\n",
    "                  [7,0,0,4,0,3,0,0,0],\n",
    "                  [0,0,0,2,0,1,0,3,0]])\n",
    "MASK = np.array([[0,1,0,0,1,0,0,0,0],\n",
    "                 [0,1,1,0,1,0,1,1,0],\n",
    "                 [0,1,1,0,1,1,0,1,0],\n",
    "                 [0,1,1,0,1,0,1,1,1],\n",
    "                 [1,1,1,0,1,0,1,0,0]])\n",
    "MASK = np.logical_not(MASK)\n",
    "\n",
    "def tp_fused_loader(tac_path, pos_path, frames):\n",
    "    output = []\n",
    "    tac_data = read_tactile_data(tac_path)\n",
    "    pos = read_pos_data(pos_path)\n",
    "    for frame in frames:\n",
    "        t = tac_data[int(frame),:]\n",
    "        p = pos[int(frame),:]\n",
    "        t = t[TAC_IMAGE_INDEX.ravel()].reshape(5,9)\n",
    "        p = p[POS_IMAGE_INDEX.ravel()].reshape(5,9)\n",
    "        t = np.ma.array(t,mask=MASK)\n",
    "        np.ma.set_fill_value(t, 999999)\n",
    "        t = t.filled()\n",
    "        r,c = np.where(t==999999)\n",
    "        t[r,c] = p[r,c]\n",
    "        output.append(t.reshape(5,9,1))\n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 15\n",
    "tactile_path = data[index][1]\n",
    "pos_path = data[index][2]\n",
    "sensor_frames = data[index][-2]\n",
    "tac_img = tp_fused_loader(tactile_path,pos_path,sensor_frames)\n",
    "# tac_img[0].shape\n",
    "fig = plt.figure(figsize=(5,2.8),frameon=False)\n",
    "\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "\n",
    "# ax.imshow(your_image, aspect='normal')\n",
    "# fig.savefig(fname, dpi)\n",
    "# ax.imshow(tac_img[0].reshape(4,5),cmap=\"gray\")\n",
    "# # plt.axis(\"off\")\n",
    "# plt.savefig(\"../../../../t/Visual-Tactile_Dataset/tactile_image/t{}.png\".format(i))\n",
    "\n",
    "for i, img in enumerate(tac_img):\n",
    "    ax.imshow(img.reshape(5,9), cmap=\"gray\")\n",
    "    plt.savefig(\"../../../../t/Visual-Tactile_Dataset/fused_image/t{}.png\".format(i))\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing 2 channel tactile + joint angle fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 4, 5, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAFDCAYAAADyPJMJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAABLFJREFUeJzt1cEJACAQwDB1/53PJQqCJBP01z0zCwAK53UAAP8wFQAypgJAxlQAyJgKABlTASBjKgBkTAWAjKkAkDEVADKmAkDGVADImAoAGVMBIGMqAGRMBYCMqQCQMRUAMqYCQMZUAMiYCgAZUwEgYyoAZEwFgIypAJAxFQAypgJAxlQAyJgKABlTASBjKgBkTAWAjKkAkDEVADKmAkDGVADImAoAGVMBIGMqAGRMBYCMqQCQMRUAMqYCQMZUAMiYCgAZUwEgYyoAZEwFgIypAJAxFQAypgJAxlQAyJgKABlTASBjKgBkTAWAjKkAkDEVADKmAkDGVADImAoAGVMBIGMqAGRMBYCMqQCQMRUAMqYCQMZUAMiYCgAZUwEgYyoAZEwFgIypAJAxFQAypgJAxlQAyJgKABlTASBjKgBkTAWAjKkAkDEVADKmAkDGVADImAoAGVMBIGMqAGRMBYCMqQCQMRUAMqYCQMZUAMiYCgAZUwEgYyoAZEwFgIypAJAxFQAypgJAxlQAyJgKABlTASBjKgBkTAWAjKkAkDEVADKmAkDGVADImAoAGVMBIGMqAGRMBYCMqQCQMRUAMqYCQMZUAMiYCgAZUwEgYyoAZEwFgIypAJAxFQAypgJAxlQAyJgKABlTASBjKgBkTAWAjKkAkDEVADKmAkDGVADImAoAGVMBIGMqAGRMBYCMqQCQMRUAMqYCQMZUAMiYCgAZUwEgYyoAZEwFgIypAJAxFQAypgJAxlQAyJgKABlTASBjKgBkTAWAjKkAkDEVADKmAkDGVADImAoAGVMBIGMqAGRMBYCMqQCQMRUAMqYCQMZUAMiYCgAZUwEgYyoAZEwFgIypAJAxFQAypgJAxlQAyJgKABlTASBjKgBkTAWAjKkAkDEVADKmAkDGVADImAoAGVMBIGMqAGRMBYCMqQCQMRUAMqYCQMZUAMiYCgAZUwEgYyoAZEwFgIypAJAxFQAypgJAxlQAyJgKABlTASBjKgBkTAWAjKkAkDEVADKmAkDGVADImAoAGVMBIGMqAGRMBYCMqQCQMRUAMqYCQMZUAMiYCgAZUwEgYyoAZEwFgIypAJAxFQAypgJAxlQAyJgKABlTASBjKgBkTAWAjKkAkDEVADKmAkDGVADImAoAGVMBIGMqAGRMBYCMqQCQMRUAMqYCQMZUAMiYCgAZUwEgYyoAZEwFgIypAJAxFQAypgJAxlQAyJgKABlTASBjKgBkTAWAjKkAkDEVADKmAkDGVADImAoAGVMBIGMqAGRMBYCMqQCQMRUAMqYCQMZUAMiYCgAZUwEgYyoAZEwFgIypAJAxFQAypgJAxlQAyJgKABlTASBjKgBkTAWAjKkAkDEVADKmAkDGVADImAoAGVMBIGMqAGRMBYCMqQCQMRUAMqYCQMZUAMiYCgAZUwEgYyoAZEwFgIypAJAxFQAypgJAxlQAyJgKABlTASBjKgBkTAWAjKkAkDEVADKmAkDGVADImAoAGVMBIGMqAGRMBYCMqQCQMRUAMqYCQMZUAMiYCgAZUwEgYyoAZEwFgIypAJAxFQAypgJAxlQAyJgKABlTASBjKgBkTAWAjKkAkDEVADIXQOcFg4RvXvUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0b4d5ba940>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TAC_IMAGE_INDEX = np.array([[14,12,13,14,15],\n",
    "                            [8,0,1,2,3],\n",
    "                            [9,4,5,6,7],\n",
    "                            [15,8,9,10,11]])\n",
    "POS_IMAGE_INDEX = np.array([[4,1,3,5,0],\n",
    "                            [5,0,2,4,1],\n",
    "                            [6,0,7,6,0],\n",
    "                            [7,1,3,5,0]])\n",
    "\n",
    "def pos_loader(pos_path, frames):\n",
    "    output =[]\n",
    "    pos = read_pos_data(pos_path)\n",
    "    for frame in frames:\n",
    "        p = pos[int(frame),:]\n",
    "        out = p[POS_IMAGE_INDEX.ravel()].reshape(4,5,1)\n",
    "        out[2,1] = 0.\n",
    "        out[2,4] = 0.\n",
    "        output.append(out)\n",
    "        # output.append(p[POS_IMAGE_INDEX.ravel()].reshape(4,4,1))\n",
    "    return np.array(output)\n",
    "\n",
    "index = 15\n",
    "tactile_path = data[index][1]\n",
    "pos_path = data[index][2]\n",
    "sensor_frames = data[index][-2]\n",
    "tac_img = tactile_loader(tactile_path,sensor_frames)\n",
    "pos_img = pos_loader(pos_path,sensor_frames)\n",
    "\n",
    "# tac_img[0].shape\n",
    "fig = plt.figure(figsize=(5,4),frameon=False)\n",
    "\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "\n",
    "\n",
    "tac_img = torch.from_numpy(tac_img)\n",
    "pos_img = torch.from_numpy(pos_img)\n",
    "\n",
    "out = torch.cat((tac_img,pos_img),dim=-1) # concatenate w.r.t. last dimension. i.e. over the channel\n",
    "np.array(out).shape\n",
    "# for i, img in enumerate(tac_img):\n",
    "#     ax.imshow(img.reshape(4,5), cmap=\"gray\")\n",
    "#     plt.savefig(\"../../../../t/Visual-Tactile_Dataset/2channelFused/tac{}.png\".format(i))\n",
    "    \n",
    "# for i, img in enumerate(pos_img):\n",
    "#     ax.imshow(img.reshape(4,5), cmap=\"gray\")\n",
    "#     plt.savefig(\"../../../../t/Visual-Tactile_Dataset/2channelFused/pos{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Resnet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import resnet\n",
    "import torch\n",
    "from torch import nn\n",
    "import dataset\n",
    "\n",
    "pretrained_path = \"../../../out/resnet/resnet-18-kinetics.pth\"\n",
    "model = resnet.resnet18(sample_size = 112, sample_duration = 18, num_classes = 400, shortcut_type='A')\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "pretrain = torch.load(pretrained_path, map_location=\"cpu\")\n",
    "model.load_state_dict(pretrain['state_dict'])\n",
    "\n",
    "x = torch.autograd.Variable(inp)\n",
    "x = x.unsqueeze(0)\n",
    "out = model(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc = nn.Linear(1024, 1)\n",
    "fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TACTILE_MAX_MAGNITUDE = np.array([23655, 20662, 14496,  6475, 41133, 64793, 59317, 33177, 19897,\n",
    "       62084, 49874, 29170, 42944, 14976, 12311, 14331])\n",
    "TAC_IMAGE_INDEX = np.array([[12,8,9,10,11],\n",
    "                          [13,0,1,2,3],\n",
    "                          [15,4,5,6,7],\n",
    "                          [14,8,9,10,11]])\n",
    "\n",
    "from scipy import signal\n",
    "TACTILE_TIME = 24\n",
    "UPSAMPLE_FREQ = 18\n",
    "up_samples=TACTILE_TIME*UPSAMPLE_FREQ\n",
    "\n",
    "\n",
    "output =[] \n",
    "with open(path1[2], 'rb') as f:\n",
    "    tactile_frame = np.loadtxt(f)\n",
    "    tactile = tactile_frame.astype('float')\n",
    "    tactile = signal.resample(tactile,up_samples)\n",
    "    tactile[np.where(tactile<0.1)] = 0.  # remove all the negative samples with 0\n",
    "    tactile = tactile/TACTILE_MAX_MAGNITUDE #normalize input in range of 0 to 1.\n",
    "    for frame in frames:\n",
    "        t = tactile[frame,:]\n",
    "        output.append(t[tac_image_index.ravel()].reshape(4,5,1))\n",
    "output = np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class tactileNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(tactileNet,self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv3d(1, 16, kernel_size=(3,3,3), padding=2),nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Conv3d(16, 32, kernel_size=(3,3,3), padding=1),nn.ReLU(),nn.MaxPool3d(kernel_size=(3,3,3)))\n",
    "        self.fc1 = nn.Sequential(nn.Linear(768, 64, bias=True), nn.ReLU(inplace=True), nn.Dropout(p = 0.5))\n",
    "        self.fc2 = nn.Linear(64,1)\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        out = self.layer1(x)\n",
    "        print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "        print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        print(out.shape)\n",
    "        out = self.fc1(out)\n",
    "        print(out.shape)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inp = torch.from_numpy(output.transpose([3,0,1,2]))\n",
    "inp = inp.unsqueeze(0)\n",
    "indata = torch.autograd.Variable(inp)\n",
    "net = tactileNet()\n",
    "net(indata.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_to_tensor(output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames = data[5][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for frame in frames:\n",
    "    output.append(tactile[frame,:].reshape(4,4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = np.arange(16,32)\n",
    "# h = np.arange(0,16)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = np.zeros((5,9))\n",
    "p = np.ones((5,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TAC_IMAGE_INDEX = np.array([[0,15,0,0,11,0,0,0,0],\n",
    "                  [0,12,8,0,9,0,10,11,0],\n",
    "                  [0,13,0,0,1,2,0,3,0],\n",
    "                  [0,14,4,0,5,0,6,7,4],\n",
    "                  [9,15,6,0,15,0,9,0,0]])\n",
    "POS_IMAGE_INDEX = np.array([[0,0,5,4,0,5,2,3,4],\n",
    "                  [4,0,0,2,0,1,0,0,0],\n",
    "                  [6,0,0,0,0,0,5,0,1],\n",
    "                  [7,0,0,4,0,3,0,0,0],\n",
    "                  [0,0,0,2,0,1,0,3,0]])\n",
    "MASK = np.array([[0,1,0,0,1,0,0,0,0],\n",
    "                 [0,1,1,0,1,0,1,1,0],\n",
    "                 [0,1,1,0,1,1,0,1,0],\n",
    "                 [0,1,1,0,1,0,1,1,1],\n",
    "                 [1,1,1,0,1,0,1,0,0]])\n",
    "m2 = np.logical_not(MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = h[TAC_IMAGE_INDEX.ravel()].reshape(5,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = np.arange(0,8)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = p[POS_IMAGE_INDEX.ravel()].reshape(5,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.ma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.ma.array([[1, 2, 3],[56,8,9]], mask = [[0, 1, 0],[1,0,0]])\n",
    "np.ma.set_fill_value(y, 9999999999)\n",
    "a = np.copy(y)\n",
    "y = y.filled()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = np.ma.array(h,mask=m2)\n",
    "np.ma.set_fill_value(h, 99)\n",
    "h = h.filled()\n",
    "r,c = np.where(h==99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h[r,c] = p[r,c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "TAC_IMAGE_INDEX = np.array([[0,15,0,0,11,0,0,0,0],\n",
    "                  [0,12,8,0,9,0,10,11,0],\n",
    "                  [0,13,0,0,1,2,0,3,0],\n",
    "                  [0,14,4,0,5,0,6,7,4],\n",
    "                  [9,15,6,0,15,0,9,0,0]])\n",
    "POS_IMAGE_INDEX = np.array([[0,0,5,4,0,5,2,3,4],\n",
    "                  [4,0,0,2,0,1,0,0,0],\n",
    "                  [6,0,0,0,0,0,5,0,1],\n",
    "                  [7,0,0,4,0,3,0,0,0],\n",
    "                  [0,0,0,2,0,1,0,3,0]])\n",
    "MASK = np.array([[0,1,0,0,1,0,0,0,0],\n",
    "                 [0,1,1,0,1,0,1,1,0],\n",
    "                 [0,1,1,0,1,1,0,1,0],\n",
    "                 [0,1,1,0,1,0,1,1,1],\n",
    "                 [1,1,1,0,1,0,1,0,0]])\n",
    "m2 = np.logical_not(MASK)\n",
    "\n",
    "h = np.arange(16,32)\n",
    "h = h[TAC_IMAGE_INDEX.ravel()].reshape(5,9)\n",
    "# print(h)\n",
    "p = np.arange(0,8)\n",
    "p = p[POS_IMAGE_INDEX.ravel()].reshape(5,9)\n",
    "# print(p)\n",
    "\n",
    "h = np.ma.array(h,mask=m2)\n",
    "np.ma.set_fill_value(h, 999999)\n",
    "h = h.filled()\n",
    "r,c = np.where(h==999999)\n",
    "h[r,c] = p[r,c]\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class tactileNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(tactileNet,self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv3d(1, 16, kernel_size=(3,3,3), padding=2),nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Conv3d(16, 32, kernel_size=(3,3,3), padding=1),nn.ReLU(),nn.MaxPool3d(kernel_size=(3,3,3)))\n",
    "        self.fc1 = nn.Sequential(nn.Linear(1152, 64, bias=True), nn.ReLU(inplace=True), nn.Dropout(p = 0.5))\n",
    "        self.fc2 = nn.Linear(64,1)\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        out = self.layer1(x)\n",
    "        print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "        print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        print(out.shape)\n",
    "        out = self.fc1(out)\n",
    "        print(out.shape)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 18, 5, 9])\n",
      "torch.Size([1, 16, 20, 7, 11])\n",
      "torch.Size([1, 32, 6, 2, 3])\n",
      "torch.Size([1, 1152])\n",
      "torch.Size([1, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0033]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(45*18).reshape(1,1,18,5,9)\n",
    "net = tactileNet()\n",
    "net(x.float())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
