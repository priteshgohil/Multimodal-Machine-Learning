{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/L1aoXingyu/pytorch-beginner/blob/master/08-AutoEncoder/Variational_autoencoder.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REad it once:\n",
    "    \n",
    "https://jhui.github.io/2018/02/09/PyTorch-Data-loading-preprocess_torchvision/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "if not os.path.exists('./mlp_img'):\n",
    "    os.mkdir('./mlp_img')\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# ransforms are common image transformations. They can be chained together using Compose\n",
    "# transforms (list of Transform objects) – list of transforms to compose.\n",
    "# img_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),  # convert numpy array to tensor, It converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #normalizes a tensor image with mean and standard deviation.\n",
    "# ])\n",
    "\n",
    "\n",
    "# img_transform1 = transforms.Compose([\n",
    "#     transforms.Resize(784),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Lambda(lambda x: x.repeat(3,1,1)),\n",
    "#     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "# transforms (list of Transform objects) – list of transforms to compose.\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # convert numpy array to tensor, It converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "    transforms.Normalize((0.5,), (0.5,)) #normalizes a tensor image with mean and standard deviation.\n",
    "])     \n",
    "\n",
    "# All the datasets have almost similar API. They all have two common arguments:\n",
    "# transform and target_transform to transform the input and target respectively.\n",
    "dataset = MNIST('./data', transform=img_transform, download=True)\n",
    "# Data loader. Combines a dataset and a sampler, and provides single- \n",
    "# or multi-process iterators over the dataset.\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - zero',\n",
       " '1 - one',\n",
       " '2 - two',\n",
       " '3 - three',\n",
       " '4 - four',\n",
       " '5 - five',\n",
       " '6 - six',\n",
       " '7 - seven',\n",
       " '8 - eight',\n",
       " '9 - nine']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pritesh/anaconda3/lib/python3.6/site-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: ./data\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=(0.5,), std=(0.5,))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
       " 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
       " 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
       " 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nn.Module is Base class for all neural network modules.\n",
    "# Your models should also subclass this class. thats why creating child\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential( #stack of network in sequence\n",
    "            nn.Linear(28 * 28, 128),  # 28*28 input image and 128 output size\n",
    "            nn.ReLU(True),            # Activation function, decide how much to weight each input\n",
    "            nn.Linear(128, 64),       #128 -> 64 dimension\n",
    "            nn.ReLU(True), \n",
    "            nn.Linear(64, 12),        # 64-> 12 dimension\n",
    "            nn.ReLU(True), \n",
    "            nn.Linear(12, 3))         # finally 12 -> 3 dimension\n",
    "        self.decoder = nn.Sequential( #performing reverse step to produce the original signal output\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True), nn.Linear(128, 28 * 28), nn.Tanh())\n",
    "        \n",
    "    #Defines the computation performed at every call.\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(torch.cuda.is_available()):\n",
    "    model = autoencoder().cuda()\n",
    "else:\n",
    "    model = autoencoder()\n",
    "# nn.MSELoss() measures the mean squared error (squared L2 norm) between\n",
    "# each element in the input xxx and target yyy.\n",
    "criterion = nn.MSELoss() \n",
    "# Implements Adam algorithm.\n",
    "# It has been proposed in Adam: A Method for Stochastic Optimization.\n",
    "optimizer = torch.optim.Adam(\n",
    "model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Linear(in_features=64, out_features=12, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Linear(in_features=12, out_features=3, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=12, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Linear(in_features=12, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Linear(in_features=128, out_features=784, bias=True)\n",
       "    (7): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need terminologies like epochs, batch size, iterations only when the data is too big which happens all the time in machine learning and we can’t pass all the data to the computer at once. So, to overcome this problem we need to divide the data into smaller sizes and give it to our computer one by one and update the weights of the neural networks at the end of every step to fit it to the data given.\n",
    "\n",
    "* Epoch : One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.\n",
    "\n",
    "* Batch : Since one epoch is too big to feed to the computer at once we divide it in several smaller batches.\n",
    "\n",
    "* Batch size: Total number of training examples present in a single batch.\n",
    "\n",
    "* Iteration: Iterations is the number of batches needed to complete one epoch.\n",
    "\n",
    "Example : We can divide the dataset of 2000 examples into batches of 500 then it will take 4 iterations to complete 1 epoch.\n",
    "\n",
    "source : https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.1609\n",
      "epoch [2/100], loss:0.1679\n",
      "epoch [3/100], loss:0.1465\n",
      "epoch [4/100], loss:0.1447\n",
      "epoch [5/100], loss:0.1572\n",
      "epoch [6/100], loss:0.1553\n",
      "epoch [7/100], loss:0.1524\n",
      "epoch [8/100], loss:0.1389\n",
      "epoch [9/100], loss:0.1474\n",
      "epoch [10/100], loss:0.1454\n",
      "epoch [11/100], loss:0.1328\n",
      "epoch [12/100], loss:0.1360\n",
      "epoch [13/100], loss:0.1325\n",
      "epoch [14/100], loss:0.1482\n",
      "epoch [15/100], loss:0.1283\n",
      "epoch [16/100], loss:0.1264\n",
      "epoch [17/100], loss:0.1354\n",
      "epoch [18/100], loss:0.1373\n",
      "epoch [19/100], loss:0.1342\n",
      "epoch [20/100], loss:0.1349\n",
      "epoch [21/100], loss:0.1249\n",
      "epoch [22/100], loss:0.1290\n",
      "epoch [23/100], loss:0.1299\n",
      "epoch [24/100], loss:0.1365\n",
      "epoch [25/100], loss:0.1194\n",
      "epoch [26/100], loss:0.1240\n",
      "epoch [27/100], loss:0.1273\n",
      "epoch [28/100], loss:0.1266\n",
      "epoch [29/100], loss:0.1163\n",
      "epoch [30/100], loss:0.1268\n",
      "epoch [31/100], loss:0.1300\n",
      "epoch [32/100], loss:0.1341\n",
      "epoch [33/100], loss:0.1306\n",
      "epoch [34/100], loss:0.1335\n",
      "epoch [35/100], loss:0.1258\n",
      "epoch [36/100], loss:0.1248\n",
      "epoch [37/100], loss:0.1147\n",
      "epoch [38/100], loss:0.1301\n",
      "epoch [39/100], loss:0.1286\n",
      "epoch [40/100], loss:0.1241\n",
      "epoch [41/100], loss:0.1327\n",
      "epoch [42/100], loss:0.1241\n",
      "epoch [43/100], loss:0.1284\n",
      "epoch [44/100], loss:0.1303\n",
      "epoch [45/100], loss:0.1277\n",
      "epoch [46/100], loss:0.1340\n",
      "epoch [47/100], loss:0.1167\n",
      "epoch [48/100], loss:0.1291\n",
      "epoch [49/100], loss:0.1267\n",
      "epoch [50/100], loss:0.1273\n",
      "epoch [51/100], loss:0.1233\n",
      "epoch [52/100], loss:0.1268\n",
      "epoch [53/100], loss:0.1222\n",
      "epoch [54/100], loss:0.1284\n",
      "epoch [55/100], loss:0.1142\n",
      "epoch [56/100], loss:0.1377\n",
      "epoch [57/100], loss:0.1254\n",
      "epoch [58/100], loss:0.1270\n",
      "epoch [59/100], loss:0.1283\n",
      "epoch [60/100], loss:0.1235\n",
      "epoch [61/100], loss:0.1194\n",
      "epoch [62/100], loss:0.1279\n",
      "epoch [63/100], loss:0.1273\n",
      "epoch [64/100], loss:0.1257\n",
      "epoch [65/100], loss:0.1205\n",
      "epoch [66/100], loss:0.1093\n",
      "epoch [67/100], loss:0.1149\n",
      "epoch [68/100], loss:0.1247\n",
      "epoch [69/100], loss:0.1192\n",
      "epoch [70/100], loss:0.1187\n",
      "epoch [71/100], loss:0.1363\n",
      "epoch [72/100], loss:0.1355\n",
      "epoch [73/100], loss:0.1199\n",
      "epoch [74/100], loss:0.1185\n",
      "epoch [75/100], loss:0.1142\n",
      "epoch [76/100], loss:0.1276\n",
      "epoch [77/100], loss:0.1256\n",
      "epoch [78/100], loss:0.1139\n",
      "epoch [79/100], loss:0.1287\n",
      "epoch [80/100], loss:0.1164\n",
      "epoch [81/100], loss:0.1264\n",
      "epoch [82/100], loss:0.1176\n",
      "epoch [83/100], loss:0.1377\n",
      "epoch [84/100], loss:0.1299\n",
      "epoch [85/100], loss:0.1104\n",
      "epoch [86/100], loss:0.1270\n",
      "epoch [87/100], loss:0.1245\n",
      "epoch [88/100], loss:0.1250\n",
      "epoch [89/100], loss:0.1129\n",
      "epoch [90/100], loss:0.1152\n",
      "epoch [91/100], loss:0.1301\n",
      "epoch [92/100], loss:0.1260\n",
      "epoch [93/100], loss:0.1244\n",
      "epoch [94/100], loss:0.1160\n",
      "epoch [95/100], loss:0.1254\n",
      "epoch [96/100], loss:0.1180\n",
      "epoch [97/100], loss:0.1184\n",
      "epoch [98/100], loss:0.1195\n",
      "epoch [99/100], loss:0.1273\n",
      "epoch [100/100], loss:0.1188\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        if(torch.cuda.is_available()):\n",
    "            img = Variable(img).cuda()\n",
    "        else:\n",
    "            img = Variable(img)\n",
    "        # ===================forward=====================\n",
    "        output = model(img) #input image to the encoder and decoder generates original image\n",
    "        loss = criterion(output, img) # calculate MSE loss function with actual image\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step() #updates the parameters\n",
    "    # ===================log========================\n",
    "#     print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.data[0]))\n",
    "#     loss.data[0] is outdated with 0.4 version, now use data.item()\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        save_image(pic, './mlp_img/image_{}.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error was caused by the shape mismatch between the tensor and self.mean in F.normalize, which tensor was [1,28,28] and self.mean was [0.5, 0.5, 0.5], so the shape of self.mean implied that the tensor should be [3, *, *], instead of [1, *, *]. So I think there is something wrong with this input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './sim_autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
